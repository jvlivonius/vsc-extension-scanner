# v3.5.3 Roadmap: Testing Excellence

**Version:** 3.5.3
**Status:** 🎯 PLANNED
**Timeline:** 4 weeks (comprehensive approach)
**Effort:** 160 hours total
**Cost:** $0 (internal quality work)

---

## Executive Summary

v3.5.3 focuses on **testing excellence** by fixing property-based tests and systematically increasing code coverage from 52.07% to 70-72%. This establishes a solid foundation for the long-term 85% coverage goal while ensuring all security-critical functionality is thoroughly validated.

**Key Objectives:**
- ✅ Fix property-based tests (21 tests, 21,000+ scenarios)
- ✅ Increase overall coverage from 52.07% to 70-72% (+18-20%)
- ✅ Create 4 missing test files (output_formatter, config_manager, extension_discovery, and expansions)
- ✅ Expand CLI testing from 17.55% to 70% (highest security ROI)
- ✅ Maintain 100% test pass rate and zero security vulnerabilities

**Deferred to v3.6+:**
- HTML report testing (0% → 60%, requires manual browser validation)
- Push beyond 72% toward 85% aspirational goal

---

## Current State Analysis

### Coverage Baseline (v3.5.2)

| Module | LOC | Coverage | Priority |
|--------|-----|----------|----------|
| html_report_generator.py | 2,373 | 0.00% | DEFERRED (v3.6+) |
| cli.py | 1,419 | 17.55% | 🔴 HIGH |
| config_manager.py | 507 | 57.48% | 🟡 MEDIUM |
| cache_manager.py | 1,422 | 58.70% | 🟡 MEDIUM |
| scanner.py | 1,469 | 60.29% | 🟡 MEDIUM |
| output_formatter.py | 415 | 62.30% | 🟡 MEDIUM |
| utils.py | 651 | 63.74% | 🟢 GOOD |
| extension_discovery.py | 387 | 63.98% | 🟡 MEDIUM |
| vscan_api.py | 973 | 77.99% | 🟢 GOOD |
| display.py | 800 | 80.58% | 🟢 GOOD |
| constants.py | 28 | 100.00% | ✅ COMPLETE |
| types.py | 19 | 100.00% | ✅ COMPLETE |
| **TOTAL** | **11,761** | **52.07%** | **TARGET: 70%** |

### Test Suite Inventory

**Current State:**
- **307 tests** across 28 test files
- **100% pass rate** ✅
- **Test code:** ~10,000 LOC
- **Property tests:** BLOCKED (missing hypothesis dependency)

**Strengths:**
- ✅ Strong security testing (6 files, 95%+ security module coverage)
- ✅ Good architecture validation (0 violations)
- ✅ Comprehensive integration tests
- ✅ Property-based testing framework in place

**Weaknesses:**
- ⚠️ html_report_generator.py: 0% coverage (2,373 LOC untested) - **DEFERRED**
- ⚠️ cli.py: 17.55% coverage (user input validation gaps)
- ⚠️ Property tests blocked (missing hypothesis)
- ⚠️ No dedicated tests for output_formatter, config_manager, extension_discovery

### Coverage Gap Analysis

**To reach 70% coverage:**
- **Current:** 6,121 LOC tested (52.07%)
- **Target:** 8,233 LOC tested (70%)
- **Gap:** 2,112 LOC requiring new tests
- **Estimated test code:** ~1,130 LOC new tests

**ROI-Ranked Priorities:**
1. **CLI expansion** (17.55% → 70%): +6.2% overall coverage
2. **Output formatter** (62.30% → 80%): +0.6% overall coverage
3. **Cache manager** (58.70% → 75%): +1.9% overall coverage
4. **Config manager** (57.48% → 75%): +0.7% overall coverage
5. **Extension discovery** (63.98% → 80%): +0.5% overall coverage
6. **Scanner** (60.29% → 70%): +1.4% overall coverage
7. **Utils** (63.74% → 75%): +0.6% overall coverage

**Total Achievable:** +11.9% → 64% baseline, stretch to 72% with polish

---

## 4-Week Implementation Plan

### Phase 1: Foundation (Week 1, 40 hours)

**Objective:** Fix property-based tests and establish baseline validation

#### Task 1.1: Property Test Dependency Fix (4 hours)

**Problem:** Property tests cannot run due to missing `hypothesis` module

**Solution:**
```bash
# Install dependencies
pip install -e .[test]  # Includes hypothesis>=6.0.0

# Verify installation
python3 -c "import hypothesis; print(f'Hypothesis {hypothesis.__version__} installed')"
```

**Validation:**
```bash
# Run property tests
python3 tests/test_property_validation.py -v
python3 tests/test_property_cache.py -v

# Expected: 21 property tests, 1000+ scenarios each = 21,000+ test cases
```

**Files Affected:**
- No code changes required (dependency issue only)

**Deliverable:** All 21 property tests passing

---

#### Task 1.2: Property Test Review & Documentation (8 hours)

**Objective:** Understand and document property test coverage

**Test Inventory:**

**test_property_validation.py** (13 property tests):
1. `test_validate_path_never_crashes` (1000 examples) - Robustness
2. `test_traversal_always_blocked` (1000 examples) - Security invariant
3. `test_system_paths_always_blocked` (500 examples) - Security invariant
4. `test_null_bytes_always_blocked` (500 examples) - Security invariant
5. `test_safe_relative_paths_accepted` (500 examples) - Functional correctness
6. `test_unicode_handled_correctly` (500 examples) - Edge case handling
7. `test_sanitize_never_crashes` (1000 examples) - Robustness
8. `test_sanitize_removes_control_chars` (1000 examples) - Security invariant
9. `test_sanitize_maintains_string_type` (500 examples) - Type safety
10. `test_sanitize_not_empty_unless_input_empty` (500 examples) - Functional correctness
11. `test_log_context_prevents_log_injection` (500 examples) - Security invariant
12. `test_safe_strings_minimally_modified` (500 examples) - Functional correctness
13. `test_idempotent_sanitization` (500 examples) - Mathematical property
14. `test_path_then_sanitize` (300 examples) - Combined security scenario

**test_property_cache.py** (8 property tests):
1. `test_cache_roundtrip_preserves_data` (500 examples) - Data integrity
2. `test_tampering_always_detected` (300 examples) - CRITICAL security invariant
3. `test_hmac_prevents_score_manipulation` (300 examples) - Security invariant
4. `test_expired_entries_not_returned` (200 examples) - Expiration logic
5. `test_cache_stats_accuracy` (200 examples) - Statistics correctness
6. `test_cache_handles_malformed_timestamps` (200 examples) - Error resilience
7. `test_cache_size_limits_respected` (200 examples) - Resource management
8. `test_concurrent_cache_access_safe` (200 examples) - Thread safety

**Actions:**
- Document property test coverage in `docs/guides/TESTING.md`
- Add property test section to test execution guide
- Update CI/CD to include property tests in test run

**Deliverable:** Property tests documented and running in CI/CD

---

#### Task 1.3: Coverage Baseline Report (4 hours)

**Objective:** Generate comprehensive coverage reports for planning

**Actions:**
```bash
# Generate coverage data
coverage run -m pytest tests/ -v

# Generate reports
coverage report --sort=Cover  # Terminal report sorted by coverage
coverage html                 # HTML report in htmlcov/
coverage json                 # JSON report for programmatic access

# Analyze gaps
coverage report --show-missing --skip-covered
```

**Analysis Deliverable:**
- Coverage report showing 52.07% baseline
- Module-by-module gap analysis
- Missing line identification for priority modules
- Test strategy recommendations

---

#### Task 1.4: Test Architecture Documentation (24 hours)

**Objective:** Document testing standards and patterns for new test development

**Deliverables:**

**1. Test File Template** (`docs/guides/TEST_FILE_TEMPLATE.md`):
```python
#!/usr/bin/env python3
"""
Test suite for [MODULE_NAME].

Coverage Target: [X]%
Priority: [HIGH|MEDIUM|LOW]

Test Categories:
- Unit tests: Core functionality validation
- Error path tests: Exception and edge case handling
- Integration tests: Cross-module interaction
- Property tests: Invariant validation (if applicable)

Part of v3.5.3 Testing Excellence initiative.
"""

import sys
import os
import unittest
from unittest.mock import patch, Mock, MagicMock

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Project imports
from vscode_scanner.[module] import [functions]


class Test[ModuleName]Unit(unittest.TestCase):
    """Unit tests for [module] core functionality."""

    def setUp(self):
        """Set up test fixtures."""
        pass

    def tearDown(self):
        """Clean up after tests."""
        pass

    def test_[function]_success(self):
        """Test [function] with valid input."""
        # ARRANGE

        # ACT

        # ASSERT
        pass


class Test[ModuleName]ErrorPaths(unittest.TestCase):
    """Error path and edge case tests for [module]."""

    def test_[function]_invalid_input(self):
        """Test [function] handles invalid input gracefully."""
        pass


def run_tests():
    """Run all tests in this file."""
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()

    suite.addTests(loader.loadTestsFromTestCase(Test[ModuleName]Unit))
    suite.addTests(loader.loadTestsFromTestCase(Test[ModuleName]ErrorPaths))

    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    return 0 if result.wasSuccessful() else 1


if __name__ == "__main__":
    sys.exit(run_tests())
```

**2. Testing Standards Update** (`docs/guides/TESTING.md` additions):
- AAA pattern (Arrange-Act-Assert) enforcement
- Mock usage guidelines
- Coverage target by module criticality
- Test naming conventions
- Error path testing requirements

**3. Coverage Strategy Document** (`docs/guides/COVERAGE_STRATEGY.md`):
- Module prioritization framework
- ROI calculation methodology
- Test effort estimation
- Coverage goal progression (52% → 70% → 85%)

---

### Phase 2: CLI Testing Expansion (Week 2, 40 hours)

**Objective:** Expand CLI test coverage from 17.55% to 70% (+6.2% overall coverage)

**Why Critical:**
- CLI is primary user interface (security boundary)
- Input validation prevents command injection
- Configuration and cache management exposed
- Highest ROI for coverage improvement

---

#### Task 2.1: Input Validator Tests (8 hours)

**File:** `tests/test_cli.py` (expand existing)

**Coverage Target:** Input validation functions

**Tests to Add:**

```python
class TestCLIValidators(unittest.TestCase):
    """Tests for CLI input validators."""

    def test_bounded_int_validator_valid_range(self):
        """Test bounded_int_validator accepts values within range."""
        validator = cli.bounded_int_validator(1, 10, "workers")
        self.assertEqual(validator("5"), 5)
        self.assertEqual(validator("1"), 1)
        self.assertEqual(validator("10"), 10)

    def test_bounded_int_validator_below_min(self):
        """Test bounded_int_validator rejects values below minimum."""
        validator = cli.bounded_int_validator(1, 10, "workers")
        with self.assertRaises(typer.BadParameter) as ctx:
            validator("0")
        self.assertIn("must be between 1 and 10", str(ctx.exception))

    def test_bounded_int_validator_above_max(self):
        """Test bounded_int_validator rejects values above maximum."""
        validator = cli.bounded_int_validator(1, 10, "workers")
        with self.assertRaises(typer.BadParameter) as ctx:
            validator("11")
        self.assertIn("must be between 1 and 10", str(ctx.exception))

    def test_bounded_int_validator_invalid_format(self):
        """Test bounded_int_validator rejects non-integer input."""
        validator = cli.bounded_int_validator(1, 10, "workers")
        with self.assertRaises(typer.BadParameter) as ctx:
            validator("abc")
        self.assertIn("must be an integer", str(ctx.exception))

    def test_bounded_float_validator_valid_range(self):
        """Test bounded_float_validator accepts values within range."""
        validator = cli.bounded_float_validator(0.0, 10.0, "delay")
        self.assertEqual(validator("5.5"), 5.5)
        self.assertEqual(validator("0.0"), 0.0)
        self.assertEqual(validator("10.0"), 10.0)

    def test_bounded_float_validator_below_min(self):
        """Test bounded_float_validator rejects values below minimum."""
        validator = cli.bounded_float_validator(0.0, 10.0, "delay")
        with self.assertRaises(typer.BadParameter) as ctx:
            validator("-0.1")
        self.assertIn("must be between 0.0 and 10.0", str(ctx.exception))

    def test_bounded_float_validator_invalid_format(self):
        """Test bounded_float_validator rejects non-numeric input."""
        validator = cli.bounded_float_validator(0.0, 10.0, "delay")
        with self.assertRaises(typer.BadParameter) as ctx:
            validator("abc")
        self.assertIn("must be a number", str(ctx.exception))
```

**Expected Coverage Impact:** +2.1% overall

---

#### Task 2.2: Config Command Tests (12 hours)

**Coverage Target:** Config subcommand functions

**Tests to Add:**

```python
class TestConfigCommands(unittest.TestCase):
    """Tests for config subcommands."""

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_init_creates_default_config(self, mock_config_class):
        """Test config init creates default configuration file."""
        mock_config = Mock()
        mock_config_class.return_value = mock_config

        # ACT
        cli.config_init()

        # ASSERT
        mock_config.create_default_config.assert_called_once()

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_show_displays_current_config(self, mock_config_class):
        """Test config show displays current configuration."""
        mock_config = Mock()
        mock_config.config = {
            'scan': {'workers': 3, 'delay': 1.0},
            'cache': {'enabled': True}
        }
        mock_config_class.return_value = mock_config

        # ACT
        with patch('builtins.print') as mock_print:
            cli.config_show()

        # ASSERT
        mock_print.assert_called()
        # Verify output contains config values

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_set_updates_value(self, mock_config_class):
        """Test config set updates configuration value."""
        mock_config = Mock()
        mock_config_class.return_value = mock_config

        # ACT
        cli.config_set("scan.workers", "5")

        # ASSERT
        mock_config.set.assert_called_once_with("scan.workers", "5")
        mock_config.save.assert_called_once()

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_set_invalid_key(self, mock_config_class):
        """Test config set rejects invalid configuration key."""
        mock_config = Mock()
        mock_config.set.side_effect = ValueError("Invalid key")
        mock_config_class.return_value = mock_config

        # ACT & ASSERT
        with self.assertRaises(typer.BadParameter):
            cli.config_set("invalid.key", "value")

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_get_retrieves_value(self, mock_config_class):
        """Test config get retrieves configuration value."""
        mock_config = Mock()
        mock_config.get.return_value = 3
        mock_config_class.return_value = mock_config

        # ACT
        with patch('builtins.print') as mock_print:
            cli.config_get("scan.workers")

        # ASSERT
        mock_config.get.assert_called_once_with("scan.workers")
        mock_print.assert_called_with(3)

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_reset_restores_defaults(self, mock_config_class):
        """Test config reset restores default configuration."""
        mock_config = Mock()
        mock_config_class.return_value = mock_config

        # ACT
        with patch('typer.confirm', return_value=True):
            cli.config_reset()

        # ASSERT
        mock_config.reset_to_defaults.assert_called_once()

    @patch('vscode_scanner.cli.ConfigManager')
    def test_config_reset_cancelled_by_user(self, mock_config_class):
        """Test config reset cancelled when user declines confirmation."""
        mock_config = Mock()
        mock_config_class.return_value = mock_config

        # ACT
        with patch('typer.confirm', return_value=False):
            cli.config_reset()

        # ASSERT
        mock_config.reset_to_defaults.assert_not_called()
```

**Expected Coverage Impact:** +2.3% overall

---

#### Task 2.3: Cache Command Tests (8 hours)

**Coverage Target:** Cache subcommand functions

**Tests to Add:**

```python
class TestCacheCommands(unittest.TestCase):
    """Tests for cache subcommands."""

    @patch('vscode_scanner.cli.CacheManager')
    def test_cache_stats_displays_statistics(self, mock_cache_class):
        """Test cache stats displays cache statistics."""
        mock_cache = Mock()
        mock_cache.get_stats.return_value = {
            'total_entries': 100,
            'cache_hits': 80,
            'cache_misses': 20,
            'hit_rate': 0.8,
            'cache_size_mb': 5.2
        }
        mock_cache_class.return_value = mock_cache

        # ACT
        with patch('vscode_scanner.display.print_cache_stats') as mock_print:
            cli.cache_stats()

        # ASSERT
        mock_cache.get_stats.assert_called_once()
        mock_print.assert_called_once()

    @patch('vscode_scanner.cli.CacheManager')
    def test_cache_clear_removes_all_entries(self, mock_cache_class):
        """Test cache clear removes all cache entries."""
        mock_cache = Mock()
        mock_cache_class.return_value = mock_cache

        # ACT
        with patch('typer.confirm', return_value=True):
            cli.cache_clear()

        # ASSERT
        mock_cache.clear.assert_called_once()

    @patch('vscode_scanner.cli.CacheManager')
    def test_cache_clear_cancelled_by_user(self, mock_cache_class):
        """Test cache clear cancelled when user declines confirmation."""
        mock_cache = Mock()
        mock_cache_class.return_value = mock_cache

        # ACT
        with patch('typer.confirm', return_value=False):
            cli.cache_clear()

        # ASSERT
        mock_cache.clear.assert_not_called()

    @patch('vscode_scanner.cli.CacheManager')
    def test_cache_clear_with_force_flag(self, mock_cache_class):
        """Test cache clear with --force skips confirmation."""
        mock_cache = Mock()
        mock_cache_class.return_value = mock_cache

        # ACT
        cli.cache_clear(force=True)

        # ASSERT
        mock_cache.clear.assert_called_once()
        # No confirmation prompt expected
```

**Expected Coverage Impact:** +1.2% overall

---

#### Task 2.4: Report Command Tests (8 hours)

**Coverage Target:** Report generation command

**Tests to Add:**

```python
class TestReportCommand(unittest.TestCase):
    """Tests for report generation command."""

    @patch('vscode_scanner.cli.CacheManager')
    @patch('vscode_scanner.cli.HTMLReportGenerator')
    def test_report_generates_html_from_cache(self, mock_html, mock_cache_class):
        """Test report generates HTML report from cache data."""
        mock_cache = Mock()
        mock_cache.get_all_results.return_value = [
            {'id': 'ext1', 'score': 8.5, 'vulnerabilities': []},
            {'id': 'ext2', 'score': 6.0, 'vulnerabilities': [{'severity': 'high'}]}
        ]
        mock_cache_class.return_value = mock_cache

        # ACT
        cli.report("output.html")

        # ASSERT
        mock_cache.get_all_results.assert_called_once()
        mock_html.return_value.generate.assert_called_once()

    @patch('vscode_scanner.cli.CacheManager')
    def test_report_empty_cache_error(self, mock_cache_class):
        """Test report command fails gracefully with empty cache."""
        mock_cache = Mock()
        mock_cache.get_all_results.return_value = []
        mock_cache_class.return_value = mock_cache

        # ACT & ASSERT
        with self.assertRaises(typer.Exit):
            cli.report("output.html")

    @patch('vscode_scanner.cli.CacheManager')
    @patch('vscode_scanner.cli.output_formatter')
    def test_report_generates_json_format(self, mock_formatter, mock_cache_class):
        """Test report generates JSON format when specified."""
        mock_cache = Mock()
        mock_cache.get_all_results.return_value = [{'id': 'ext1'}]
        mock_cache_class.return_value = mock_cache

        # ACT
        cli.report("output.json", format="json")

        # ASSERT
        mock_formatter.format_json.assert_called_once()

    @patch('vscode_scanner.cli.CacheManager')
    @patch('vscode_scanner.cli.output_formatter')
    def test_report_generates_csv_format(self, mock_formatter, mock_cache_class):
        """Test report generates CSV format when specified."""
        mock_cache = Mock()
        mock_cache.get_all_results.return_value = [{'id': 'ext1'}]
        mock_cache_class.return_value = mock_cache

        # ACT
        cli.report("output.csv", format="csv")

        # ASSERT
        mock_formatter.format_csv.assert_called_once()
```

**Expected Coverage Impact:** +0.6% overall

---

#### Task 2.5: Error Handling Tests (4 hours)

**Coverage Target:** CLI error paths and edge cases

**Tests to Add:**

```python
class TestCLIErrorHandling(unittest.TestCase):
    """Tests for CLI error handling."""

    @patch('vscode_scanner.cli.scanner')
    def test_scan_keyboard_interrupt_handled(self, mock_scanner):
        """Test scan handles KeyboardInterrupt gracefully."""
        mock_scanner.run_scan.side_effect = KeyboardInterrupt()

        # ACT & ASSERT
        with self.assertRaises(typer.Exit) as ctx:
            cli.scan()
        self.assertEqual(ctx.exception.exit_code, 130)

    @patch('vscode_scanner.cli.scanner')
    def test_scan_permission_error_handled(self, mock_scanner):
        """Test scan handles PermissionError gracefully."""
        mock_scanner.run_scan.side_effect = PermissionError("Access denied")

        # ACT & ASSERT
        with self.assertRaises(typer.Exit) as ctx:
            cli.scan()
        self.assertEqual(ctx.exception.exit_code, 2)

    def test_invalid_output_path_handled(self):
        """Test scan validates output path before execution."""
        # ACT & ASSERT
        with self.assertRaises(typer.BadParameter):
            cli.scan(output="/invalid/../../etc/passwd")
```

**Expected Coverage Impact:** +0.5% overall

---

**Phase 2 Summary:**
- **Effort:** 40 hours
- **New Test Code:** ~500 LOC
- **Coverage Impact:** cli.py 17.55% → 70% (+6.2% overall)
- **Tests Added:** ~35 new test functions
- **Overall Coverage:** 52.07% → 58.3%

---

### Phase 3: Core Module Tests (Week 3, 40 hours)

**Objective:** Create missing test files and expand existing tests for core modules

---

#### Task 3.1: Output Formatter Tests (8 hours)

**File:** `tests/test_output_formatter.py` (CREATE NEW)

**Coverage Target:** output_formatter.py 62.30% → 80%

**Test Structure:**

```python
#!/usr/bin/env python3
"""
Test suite for output_formatter.py

Coverage Target: 80%
Priority: MEDIUM

Test Categories:
- JSON format validation
- CSV format validation
- Schema version validation
- Error handling

Part of v3.5.3 Testing Excellence initiative.
"""

class TestJSONFormatter(unittest.TestCase):
    """Tests for JSON output formatting."""

    def test_format_json_standard_mode(self):
        """Test JSON format in standard mode."""
        results = [
            SimpleNamespace(
                id='ext1',
                display_name='Extension 1',
                version='1.0.0',
                publisher='publisher1',
                vscan_url='https://vscan.dev/ext1',
                security_score=8.5,
                verified_publisher=True,
                vulnerabilities=[]
            )
        ]

        json_output = output_formatter.format_json(results, mode='standard')
        data = json.loads(json_output)

        self.assertEqual(data['schema_version'], '2.0')
        self.assertEqual(len(data['extensions']), 1)
        self.assertEqual(data['extensions'][0]['id'], 'ext1')

    def test_format_json_detailed_mode(self):
        """Test JSON format in detailed mode includes additional fields."""
        results = [SimpleNamespace(id='ext1', ...)]

        json_output = output_formatter.format_json(results, mode='detailed')
        data = json.loads(json_output)

        self.assertIn('scan_timestamp', data)
        self.assertIn('cache_used', data['extensions'][0])

    def test_format_json_with_vulnerabilities(self):
        """Test JSON format includes vulnerability details."""
        vuln = {
            'id': 'CVE-2024-1234',
            'severity': 'high',
            'description': 'XSS vulnerability'
        }
        results = [SimpleNamespace(id='ext1', vulnerabilities=[vuln], ...)]

        json_output = output_formatter.format_json(results)
        data = json.loads(json_output)

        self.assertEqual(len(data['extensions'][0]['vulnerabilities']), 1)
        self.assertEqual(data['extensions'][0]['vulnerabilities'][0]['id'], 'CVE-2024-1234')

    def test_format_json_empty_results(self):
        """Test JSON format handles empty results gracefully."""
        json_output = output_formatter.format_json([])
        data = json.loads(json_output)

        self.assertEqual(data['total_extensions'], 0)
        self.assertEqual(data['extensions'], [])


class TestCSVFormatter(unittest.TestCase):
    """Tests for CSV output formatting."""

    def test_format_csv_standard_mode(self):
        """Test CSV format in standard mode."""
        results = [SimpleNamespace(id='ext1', security_score=8.5, ...)]

        csv_output = output_formatter.format_csv(results, mode='standard')
        lines = csv_output.strip().split('\n')

        # Verify header
        self.assertIn('ID', lines[0])
        self.assertIn('Security Score', lines[0])

        # Verify data row
        self.assertIn('ext1', lines[1])
        self.assertIn('8.5', lines[1])

    def test_format_csv_with_commas_in_values(self):
        """Test CSV format properly escapes commas in values."""
        results = [SimpleNamespace(
            id='ext1',
            display_name='Extension, with comma',
            ...
        )]

        csv_output = output_formatter.format_csv(results)

        # Verify comma is escaped
        self.assertIn('"Extension, with comma"', csv_output)

    def test_format_csv_with_quotes_in_values(self):
        """Test CSV format properly escapes quotes in values."""
        results = [SimpleNamespace(
            id='ext1',
            display_name='Extension "quoted" name',
            ...
        )]

        csv_output = output_formatter.format_csv(results)

        # Verify quotes are doubled
        self.assertIn('Extension ""quoted"" name', csv_output)


class TestSchemaVersion(unittest.TestCase):
    """Tests for schema version validation."""

    def test_schema_version_current(self):
        """Test current schema version is 2.0."""
        self.assertEqual(output_formatter.SCHEMA_VERSION, '2.0')

    def test_schema_version_in_json_output(self):
        """Test schema version is included in JSON output."""
        json_output = output_formatter.format_json([])
        data = json.loads(json_output)

        self.assertIn('schema_version', data)
        self.assertEqual(data['schema_version'], '2.0')


class TestErrorHandling(unittest.TestCase):
    """Tests for error handling in output formatter."""

    def test_format_json_handles_unicode(self):
        """Test JSON format handles unicode characters."""
        results = [SimpleNamespace(
            id='ext1',
            display_name='Extension with émojis 🎉',
            ...
        )]

        json_output = output_formatter.format_json(results)
        data = json.loads(json_output)

        self.assertEqual(data['extensions'][0]['display_name'], 'Extension with émojis 🎉')

    def test_format_csv_handles_newlines(self):
        """Test CSV format handles newlines in values."""
        results = [SimpleNamespace(
            id='ext1',
            display_name='Extension\nwith\nnewlines',
            ...
        )]

        csv_output = output_formatter.format_csv(results)

        # Verify newlines are preserved in quoted field
        self.assertIn('"Extension\nwith\nnewlines"', csv_output)
```

**Expected Coverage Impact:** +0.6% overall

---

#### Task 3.2: Config Manager Tests (8 hours)

**File:** `tests/test_config_manager.py` (CREATE NEW)

**Coverage Target:** config_manager.py 57.48% → 75%

**Test Structure:**

```python
#!/usr/bin/env python3
"""
Test suite for config_manager.py

Coverage Target: 75%
Priority: MEDIUM

Test Categories:
- Config file parsing
- Config validation
- Default value handling
- Error recovery

Part of v3.5.3 Testing Excellence initiative.
"""

class TestConfigParsing(unittest.TestCase):
    """Tests for config file parsing."""

    def setUp(self):
        """Create temporary config directory."""
        self.temp_dir = tempfile.mkdtemp()
        self.config_path = Path(self.temp_dir) / ".vscanrc"

    def tearDown(self):
        """Clean up temporary files."""
        shutil.rmtree(self.temp_dir)

    def test_load_valid_config(self):
        """Test loading valid configuration file."""
        config_content = """
        [scan]
        workers = 5
        delay = 2.0

        [cache]
        enabled = true
        expiration_days = 30
        """
        self.config_path.write_text(config_content)

        manager = ConfigManager(config_path=self.config_path)

        self.assertEqual(manager.get('scan.workers'), 5)
        self.assertEqual(manager.get('scan.delay'), 2.0)
        self.assertEqual(manager.get('cache.enabled'), True)

    def test_load_config_with_comments(self):
        """Test config parsing ignores comments."""
        config_content = """
        # This is a comment
        [scan]
        workers = 3  # inline comment
        """
        self.config_path.write_text(config_content)

        manager = ConfigManager(config_path=self.config_path)

        self.assertEqual(manager.get('scan.workers'), 3)

    def test_load_config_missing_file(self):
        """Test loading non-existent config file uses defaults."""
        manager = ConfigManager(config_path=Path("/nonexistent/.vscanrc"))

        # Should use defaults
        self.assertEqual(manager.get('scan.workers'), 3)
        self.assertEqual(manager.get('scan.delay'), 1.0)


class TestConfigValidation(unittest.TestCase):
    """Tests for configuration validation."""

    def test_set_valid_value(self):
        """Test setting valid configuration value."""
        manager = ConfigManager()
        manager.set('scan.workers', 5)

        self.assertEqual(manager.get('scan.workers'), 5)

    def test_set_invalid_key(self):
        """Test setting invalid configuration key raises error."""
        manager = ConfigManager()

        with self.assertRaises(ValueError) as ctx:
            manager.set('invalid.key', 'value')

        self.assertIn('Invalid configuration key', str(ctx.exception))

    def test_set_invalid_type(self):
        """Test setting value with wrong type raises error."""
        manager = ConfigManager()

        with self.assertRaises(ValueError) as ctx:
            manager.set('scan.workers', 'not_an_integer')

        self.assertIn('must be an integer', str(ctx.exception))

    def test_set_out_of_range_value(self):
        """Test setting out-of-range value raises error."""
        manager = ConfigManager()

        with self.assertRaises(ValueError) as ctx:
            manager.set('scan.workers', 100)

        self.assertIn('must be between', str(ctx.exception))


class TestDefaultValues(unittest.TestCase):
    """Tests for default value handling."""

    def test_get_default_workers(self):
        """Test default workers value is 3."""
        manager = ConfigManager()
        self.assertEqual(manager.get('scan.workers'), 3)

    def test_get_default_delay(self):
        """Test default delay value is 1.0."""
        manager = ConfigManager()
        self.assertEqual(manager.get('scan.delay'), 1.0)

    def test_reset_to_defaults(self):
        """Test resetting configuration to defaults."""
        manager = ConfigManager()
        manager.set('scan.workers', 5)

        manager.reset_to_defaults()

        self.assertEqual(manager.get('scan.workers'), 3)


class TestErrorRecovery(unittest.TestCase):
    """Tests for error recovery in config manager."""

    def test_load_malformed_config_uses_defaults(self):
        """Test loading malformed config file falls back to defaults."""
        temp_dir = tempfile.mkdtemp()
        config_path = Path(temp_dir) / ".vscanrc"
        config_path.write_text("invalid config content {{{")

        manager = ConfigManager(config_path=config_path)

        # Should fall back to defaults
        self.assertEqual(manager.get('scan.workers'), 3)

        shutil.rmtree(temp_dir)

    def test_save_config_creates_directory(self):
        """Test saving config creates parent directory if missing."""
        temp_dir = tempfile.mkdtemp()
        config_path = Path(temp_dir) / "subdir" / ".vscanrc"

        manager = ConfigManager(config_path=config_path)
        manager.set('scan.workers', 5)
        manager.save()

        self.assertTrue(config_path.exists())

        shutil.rmtree(temp_dir)
```

**Expected Coverage Impact:** +0.7% overall

---

#### Task 3.3: Cache Manager Expansion (12 hours)

**File:** `tests/test_cache_integrity.py` (EXPAND EXISTING)

**Coverage Target:** cache_manager.py 58.70% → 75%

**Tests to Add:**

```python
class TestCacheErrorRecovery(unittest.TestCase):
    """Tests for cache error recovery paths."""

    def test_cache_handles_corrupted_database(self):
        """Test cache recovers from corrupted database file."""
        cache = CacheManager()

        # Corrupt the database file
        with open(cache.db_path, 'wb') as f:
            f.write(b'corrupted data')

        # Should recover by recreating database
        result = cache.get_result('ext1')
        self.assertIsNone(result)

    def test_cache_handles_invalid_hmac(self):
        """Test cache detects and rejects invalid HMAC signatures."""
        cache = CacheManager()

        # Store valid result
        result = SimpleNamespace(id='ext1', security_score=8.5, ...)
        cache.store_result('ext1', result)

        # Tamper with data directly in database
        conn = sqlite3.connect(cache.db_path)
        cursor = conn.cursor()
        cursor.execute("UPDATE results SET security_score = 9.9 WHERE extension_id = 'ext1'")
        conn.commit()
        conn.close()

        # Should detect tampering and reject
        retrieved = cache.get_result('ext1')
        self.assertIsNone(retrieved)

    def test_cache_handles_disk_full_error(self):
        """Test cache handles disk full errors gracefully."""
        cache = CacheManager()

        with patch('sqlite3.connect') as mock_connect:
            mock_conn = Mock()
            mock_conn.execute.side_effect = sqlite3.OperationalError("disk I/O error")
            mock_connect.return_value = mock_conn

            # Should not crash
            result = cache.get_result('ext1')
            self.assertIsNone(result)


class TestCacheExpiration(unittest.TestCase):
    """Tests for cache expiration edge cases."""

    def test_expired_entry_not_returned(self):
        """Test expired cache entries are not returned."""
        cache = CacheManager(expiration_days=30)

        # Store result with expired timestamp
        old_timestamp = datetime.now() - timedelta(days=31)

        with patch('vscode_scanner.cache_manager.datetime') as mock_datetime:
            mock_datetime.now.return_value = old_timestamp
            result = SimpleNamespace(id='ext1', security_score=8.5, ...)
            cache.store_result('ext1', result)

        # Should not return expired entry
        retrieved = cache.get_result('ext1')
        self.assertIsNone(retrieved)

    def test_cache_cleanup_removes_expired_entries(self):
        """Test cache cleanup removes expired entries."""
        cache = CacheManager(expiration_days=30)

        # Store multiple entries with different timestamps
        old = datetime.now() - timedelta(days=31)
        recent = datetime.now() - timedelta(days=1)

        # Store old entry
        with patch('vscode_scanner.cache_manager.datetime') as mock_datetime:
            mock_datetime.now.return_value = old
            cache.store_result('ext_old', SimpleNamespace(id='ext_old', ...))

        # Store recent entry
        with patch('vscode_scanner.cache_manager.datetime') as mock_datetime:
            mock_datetime.now.return_value = recent
            cache.store_result('ext_recent', SimpleNamespace(id='ext_recent', ...))

        # Run cleanup
        cache.cleanup_expired()

        # Old entry should be removed, recent should remain
        self.assertIsNone(cache.get_result('ext_old'))
        self.assertIsNotNone(cache.get_result('ext_recent'))


class TestBatchOperations(unittest.TestCase):
    """Tests for batch operation error handling."""

    def test_batch_store_partial_failure(self):
        """Test batch store handles partial failures gracefully."""
        cache = CacheManager()

        results = [
            SimpleNamespace(id='ext1', security_score=8.5, ...),
            SimpleNamespace(id='ext2', security_score=9.0, ...),
            SimpleNamespace(id='ext3', security_score=7.5, ...)
        ]

        # Mock failure on second item
        with patch.object(cache, '_store_single') as mock_store:
            mock_store.side_effect = [None, Exception("Storage error"), None]

            # Should continue despite error
            cache.store_batch(results)

        # First and third should be stored, second should fail
        self.assertIsNotNone(cache.get_result('ext1'))
        self.assertIsNone(cache.get_result('ext2'))
        self.assertIsNotNone(cache.get_result('ext3'))
```

**Expected Coverage Impact:** +1.9% overall

---

#### Task 3.4: Extension Discovery Tests (8 hours)

**File:** `tests/test_extension_discovery.py` (CREATE NEW)

**Coverage Target:** extension_discovery.py 63.98% → 80%

**Test Structure:**

```python
#!/usr/bin/env python3
"""
Test suite for extension_discovery.py

Coverage Target: 80%
Priority: MEDIUM

Test Categories:
- Extension discovery
- package.json parsing
- Error handling

Part of v3.5.3 Testing Excellence initiative.
"""

class TestExtensionDiscovery(unittest.TestCase):
    """Tests for extension discovery logic."""

    def setUp(self):
        """Create temporary extensions directory."""
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        """Clean up temporary files."""
        shutil.rmtree(self.temp_dir)

    def test_discover_valid_extensions(self):
        """Test discovering valid extensions."""
        # Create extension directories with package.json
        ext1_dir = Path(self.temp_dir) / "publisher.ext1-1.0.0"
        ext1_dir.mkdir()
        (ext1_dir / "package.json").write_text(json.dumps({
            "name": "ext1",
            "version": "1.0.0",
            "publisher": "publisher"
        }))

        extensions, stats = discover_extensions(self.temp_dir)

        self.assertEqual(len(extensions), 1)
        self.assertEqual(extensions[0]['id'], 'publisher.ext1')

    def test_discover_malformed_package_json(self):
        """Test discovery handles malformed package.json gracefully."""
        # Create extension with invalid JSON
        ext_dir = Path(self.temp_dir) / "publisher.ext1-1.0.0"
        ext_dir.mkdir()
        (ext_dir / "package.json").write_text("{invalid json")

        extensions, stats = discover_extensions(self.temp_dir)

        # Should skip malformed extension
        self.assertEqual(len(extensions), 0)
        self.assertEqual(stats['skipped'], 1)

    def test_discover_missing_package_json(self):
        """Test discovery skips directories without package.json."""
        # Create directory without package.json
        ext_dir = Path(self.temp_dir) / "publisher.ext1-1.0.0"
        ext_dir.mkdir()

        extensions, stats = discover_extensions(self.temp_dir)

        self.assertEqual(len(extensions), 0)
        self.assertEqual(stats['skipped'], 1)

    def test_discover_with_permission_error(self):
        """Test discovery handles permission errors gracefully."""
        # Create extension directory
        ext_dir = Path(self.temp_dir) / "publisher.ext1-1.0.0"
        ext_dir.mkdir()
        (ext_dir / "package.json").write_text(json.dumps({"name": "ext1"}))

        # Simulate permission error
        with patch('pathlib.Path.read_text', side_effect=PermissionError("Access denied")):
            extensions, stats = discover_extensions(self.temp_dir)

        self.assertEqual(len(extensions), 0)
        self.assertEqual(stats['skipped'], 1)


class TestPackageJsonParsing(unittest.TestCase):
    """Tests for package.json parsing."""

    def test_parse_complete_package_json(self):
        """Test parsing complete package.json with all fields."""
        package_data = {
            "name": "ext1",
            "version": "1.0.0",
            "publisher": "publisher",
            "displayName": "Extension 1",
            "description": "An extension",
            "engines": {"vscode": "^1.50.0"}
        }

        extension = parse_package_json(package_data, "path/to/ext")

        self.assertEqual(extension['name'], 'ext1')
        self.assertEqual(extension['version'], '1.0.0')
        self.assertEqual(extension['display_name'], 'Extension 1')

    def test_parse_minimal_package_json(self):
        """Test parsing minimal package.json with required fields only."""
        package_data = {
            "name": "ext1",
            "version": "1.0.0",
            "publisher": "publisher"
        }

        extension = parse_package_json(package_data, "path/to/ext")

        self.assertIsNotNone(extension)
        self.assertEqual(extension['name'], 'ext1')

    def test_parse_missing_required_fields(self):
        """Test parsing fails with missing required fields."""
        package_data = {
            "name": "ext1"
            # Missing version and publisher
        }

        with self.assertRaises(ValueError):
            parse_package_json(package_data, "path/to/ext")
```

**Expected Coverage Impact:** +0.5% overall

---

#### Task 3.5: Scanner Module Expansion (4 hours)

**File:** `tests/test_scanner.py` (EXPAND EXISTING)

**Coverage Target:** scanner.py 60.29% → 70%

**Tests to Add:**

```python
class TestScannerErrorPaths(unittest.TestCase):
    """Tests for scanner error paths."""

    @patch('vscode_scanner.scanner.extension_discovery')
    def test_scan_handles_discovery_error(self, mock_discovery):
        """Test scan handles extension discovery errors."""
        mock_discovery.discover_extensions.side_effect = Exception("Discovery failed")

        exit_code = scanner.run_scan()

        self.assertEqual(exit_code, 2)  # Error exit code

    @patch('vscode_scanner.scanner.ThreadPoolExecutor')
    def test_scan_handles_worker_thread_crash(self, mock_executor):
        """Test scan handles worker thread crashes gracefully."""
        mock_future = Mock()
        mock_future.result.side_effect = Exception("Worker crashed")

        mock_executor.return_value.__enter__.return_value.submit.return_value = mock_future

        # Should continue despite worker crash
        exit_code = scanner.run_scan()
        self.assertIsNotNone(exit_code)

    @patch('vscode_scanner.scanner.vscan_api')
    def test_scan_handles_api_timeout(self, mock_api):
        """Test scan handles API timeout errors."""
        mock_api.scan_extension.side_effect = TimeoutError("API timeout")

        exit_code = scanner.run_scan()

        # Should complete despite timeout
        self.assertIsNotNone(exit_code)
```

**Expected Coverage Impact:** +1.4% overall

---

**Phase 3 Summary:**
- **Effort:** 40 hours
- **New Test Code:** ~630 LOC
- **Test Files Created:** 3 new files (output_formatter, config_manager, extension_discovery)
- **Test Files Expanded:** 2 files (cache_integrity, scanner)
- **Coverage Impact:** +5.1% overall
- **Overall Coverage:** 58.3% → 63.4%

---

### Phase 4: Polish & Validation (Week 4, 40 hours)

**Objective:** Expand existing tests to reach 70%+ coverage and validate quality

---

#### Task 4.1: Utils Module Expansion (8 hours)

**File:** `tests/test_utils.py` (CREATE NEW for non-security utilities)

**Coverage Target:** utils.py 63.74% → 75%

**Tests to Add:**

```python
class TestUtilityFunctions(unittest.TestCase):
    """Tests for non-security utility functions."""

    def test_format_timestamp(self):
        """Test timestamp formatting utility."""
        timestamp = datetime(2024, 1, 15, 14, 30, 0)
        formatted = utils.format_timestamp(timestamp)

        self.assertEqual(formatted, "2024-01-15 14:30:00")

    def test_parse_version_string(self):
        """Test version string parsing."""
        version = "1.2.3"
        major, minor, patch = utils.parse_version(version)

        self.assertEqual(major, 1)
        self.assertEqual(minor, 2)
        self.assertEqual(patch, 3)

    def test_compare_versions(self):
        """Test version comparison utility."""
        self.assertTrue(utils.compare_versions("1.2.3", "1.2.2") > 0)
        self.assertTrue(utils.compare_versions("1.2.3", "1.2.3") == 0)
        self.assertTrue(utils.compare_versions("1.2.3", "1.2.4") < 0)
```

**Expected Coverage Impact:** +0.6% overall

---

#### Task 4.2: VScan API Polish (4 hours)

**File:** `tests/test_api.py` (EXPAND EXISTING)

**Coverage Target:** vscan_api.py 77.99% → 85%

**Tests to Add:** Rare error paths, edge cases, timeout handling

**Expected Coverage Impact:** +0.7% overall

---

#### Task 4.3: Display Module Polish (4 hours)

**File:** `tests/test_display.py` (EXPAND EXISTING)

**Coverage Target:** display.py 80.58% → 85%

**Tests to Add:** Edge cases in formatting, width calculations, unicode handling

**Expected Coverage Impact:** +0.4% overall

---

#### Task 4.4: Integration Test Expansion (8 hours)

**File:** `tests/test_integration.py` (EXPAND EXISTING)

**Objective:** Add end-to-end scenarios covering cross-module interactions

**Tests to Add:**
- Full scan with cache → report generation → cache reuse
- Config changes → scan behavior validation
- Error recovery across multiple components
- Parallel scan coordination with failures

**Expected Coverage Impact:** +0.5% overall (integration doesn't directly add coverage but validates existing code paths)

---

#### Task 4.5: Coverage Validation & Reporting (8 hours)

**Objective:** Generate comprehensive coverage reports and validate quality

**Actions:**

1. **Generate Coverage Reports:**
```bash
# Generate all report formats
coverage run -m pytest tests/ -v
coverage report --sort=Cover > coverage_report.txt
coverage html
coverage json
coverage xml  # For CI/CD integration
```

2. **Analyze Coverage by Module:**
```bash
# Identify remaining gaps
coverage report --show-missing --skip-covered

# Generate module-specific reports
coverage report --include="vscode_scanner/cli.py"
coverage report --include="vscode_scanner/cache_manager.py"
# ... etc for each module
```

3. **Validate Quality Metrics:**
- Test execution time <60s
- Zero flaky tests (run suite 10 times)
- 100% test pass rate
- Zero security vulnerabilities (Bandit, Safety, pip-audit)
- Zero architecture violations

4. **Create Coverage Dashboard:**
- HTML coverage report with module breakdowns
- Trend analysis (52% → 70%+)
- Gap identification for future work
- Recommendations for reaching 85% goal

**Deliverable:** Comprehensive coverage validation report

---

#### Task 4.6: Documentation Updates (8 hours)

**Objective:** Update all testing documentation to reflect new coverage state

**Files to Update:**

1. **docs/guides/TESTING.md:**
   - Update coverage statistics (52% → 70%+)
   - Document new test files
   - Add property test execution guide
   - Update test suite structure

2. **docs/guides/COVERAGE_STRATEGY.md** (CREATE NEW):
   - Current coverage: 70%+
   - Aspirational goal: 85% overall, 95% security
   - Module prioritization for future work
   - ROI framework for test development
   - Gap analysis for remaining 15%

3. **docs/project/STATUS.md:**
   - Update current version to v3.5.3
   - Add coverage achievement metrics
   - Update test suite statistics

4. **CLAUDE.md:**
   - Update current status section
   - Reference v3.5.3 achievements
   - Note testing excellence initiative

5. **CHANGELOG.md:**
   - Add v3.5.3 entry with coverage improvements
   - List new test files created
   - Document property test fixes

**Deliverable:** Complete documentation reflecting v3.5.3 testing improvements

---

**Phase 4 Summary:**
- **Effort:** 40 hours
- **New Test Code:** ~200 LOC
- **Coverage Impact:** +2.2% overall
- **Overall Coverage:** 63.4% → 65.6%
- **With optimizations:** Potential to reach 70-72%

---

## Success Criteria

### Coverage Targets ✅

| Metric | Current | Target | Stretch |
|--------|---------|--------|---------|
| **Overall Coverage** | 52.07% | 70% | 72% |
| **cli.py** | 17.55% | 70% | 75% |
| **output_formatter.py** | 62.30% | 80% | 85% |
| **config_manager.py** | 57.48% | 75% | 80% |
| **cache_manager.py** | 58.70% | 75% | 80% |
| **scanner.py** | 60.29% | 70% | 75% |
| **extension_discovery.py** | 63.98% | 80% | 85% |
| **utils.py** | 63.74% | 75% | 80% |

### Quality Metrics ✅

- ✅ **100% test pass rate** maintained
- ✅ **21 property tests passing** (21,000+ scenarios validated)
- ✅ **Zero security vulnerabilities** (Bandit, Safety, pip-audit)
- ✅ **Zero architecture violations** (layer compliance)
- ✅ **Test execution time <60s** (fast feedback loop)
- ✅ **No flaky tests introduced** (consistent pass rate)
- ✅ **AAA pattern compliance** (all new tests)
- ✅ **Clear test documentation** (purpose, coverage target)

### Test Suite Growth

| Metric | Current | v3.5.3 Target |
|--------|---------|---------------|
| **Test Files** | 28 | 31 |
| **Test Functions** | 307 | 370+ |
| **Test Code (LOC)** | ~10,000 | ~11,130 |
| **Property Tests** | BLOCKED | 21 passing |
| **Coverage %** | 52.07% | 70-72% |

---

## Risk Mitigation

### Technical Risks

**Risk 1: Property Tests Reveal Bugs**
- **Likelihood:** Medium
- **Impact:** High (security vulnerabilities)
- **Mitigation:**
  - Run property tests early (Phase 1)
  - Fix any discovered issues before continuing
  - May require security patches before v3.5.3 release

**Risk 2: Coverage Goals Unachievable**
- **Likelihood:** Low
- **Impact:** Medium
- **Mitigation:**
  - Conservative estimates (65% baseline, 70% target, 72% stretch)
  - Focus on high-ROI modules first (CLI, output formatter)
  - Phase approach allows early validation

**Risk 3: Test Suite Performance Degradation**
- **Likelihood:** Medium
- **Impact:** Medium (slow feedback loop)
- **Mitigation:**
  - Monitor test execution time after each phase
  - Optimize slow tests
  - Use pytest-xdist for parallel execution if needed
  - Target: maintain <60s total execution time

### Resource Risks

**Risk 4: Effort Underestimation**
- **Likelihood:** Medium
- **Impact:** Low (schedule slip)
- **Mitigation:**
  - 4-week timeline has buffer
  - Phases can be completed independently
  - Minimum viable completion: 70% coverage (Phases 1-3)
  - Phase 4 is optional polish

**Risk 5: HTML Testing Deferral Creates Debt**
- **Likelihood:** High
- **Impact:** Medium (large untested module)
- **Mitigation:**
  - Explicitly documented as deferred to v3.6+
  - html_report_generator.py remains 0% coverage
  - Separate roadmap item for v3.6 (HTML testing focus)
  - No user-facing risk (HTML output already works)

---

## Deferred to Future Versions

### v3.6: HTML Report Testing

**Scope:** html_report_generator.py 0% → 60% (+9.1% overall)

**Why Deferred:**
- Requires manual browser validation (Chrome, Firefox, Safari, Edge)
- Visual testing tools needed (screenshot comparison)
- JavaScript testing framework required
- High effort (5-7 days)
- Lower priority than CLI security testing

**Rationale:**
- Focus v3.5.3 on highest security ROI (CLI, config, cache)
- HTML output is user-facing but non-security-critical
- Existing HTML output works correctly (just untested)
- Deferral allows 70% goal without manual testing overhead

**v3.6 Plan:**
- Automated HTML structure validation
- JavaScript functionality testing
- XSS prevention validation
- Browser compatibility testing (manual)
- Visual regression testing (optional)

### v3.7: Push to 85% Coverage

**Scope:** 70% → 85% (+15% coverage)

**Modules to Improve:**
- html_report_generator.py: 60% → 85%
- cli.py: 70% → 85%
- All modules: Polish to 85%+

**Effort:** 3-4 weeks

**Why Deferred:**
- Diminishing returns (Pareto principle: 70% covers 95% of critical paths)
- v3.5.3 achieves "good enough" coverage for quality
- Remaining 15% is edge cases and rare error paths
- Better to validate 70% works well before pushing higher

---

## Lessons Learned from v3.5.2

### What Worked Well ✅

1. **Phased Approach:**
   - Incremental progress easier to validate
   - Phases can complete independently
   - Early wins build momentum

2. **Tool Implementation:**
   - Property-based testing framework valuable
   - Coverage infrastructure essential
   - CI/CD automation prevents regression

3. **Documentation Quality:**
   - Comprehensive roadmaps guide execution
   - Clear success criteria enable validation
   - Honest metrics build credibility

### Challenges Overcome ⚠️

1. **Coverage Expectations:**
   - Initial claim: "85% coverage achieved"
   - Reality: 52% baseline, 85% aspirational goal
   - **Lesson:** Measure before claiming, be honest about baselines

2. **Property Test Dependency:**
   - Property tests blocked by missing hypothesis
   - **Lesson:** Validate dependencies early in Phase 1
   - **Applied:** Phase 1 Task 1.1 addresses this immediately

3. **Scope Management:**
   - v3.5.2 deferred 12 tools to future versions
   - **Lesson:** Focus on high-impact work, defer low-ROI items
   - **Applied:** HTML testing deferred to v3.6

### Applied to v3.5.3 ✅

1. **Conservative Coverage Targets:**
   - Target: 70% (achievable)
   - Stretch: 72% (buffer)
   - Aspirational: 85% (long-term)

2. **Early Validation:**
   - Phase 1: Fix property tests immediately
   - Validate coverage baseline before building

3. **Honest Communication:**
   - Clear about what's deferred (HTML testing)
   - Realistic effort estimates (4 weeks)
   - ROI-driven prioritization

---

## Timeline Summary

### Gantt Chart

```
Week 1: Foundation
  Mon-Tue:  Property test fix & validation
  Wed-Fri:  Test architecture & documentation

Week 2: CLI Testing
  Mon-Tue:  Input validator tests
  Wed-Thu:  Config & cache command tests
  Fri:      Report command & error handling tests

Week 3: Core Modules
  Mon-Tue:  Output formatter tests
  Wed:      Config manager tests
  Thu-Fri:  Cache manager & extension discovery tests

Week 4: Polish
  Mon:      Utils & API polish
  Tue:      Integration test expansion
  Wed:      Coverage validation
  Thu-Fri:  Documentation updates
```

### Milestones

- **Week 1 Complete:** Property tests passing, test architecture documented
- **Week 2 Complete:** CLI coverage 70%, overall 58%
- **Week 3 Complete:** Core modules improved, overall 64%
- **Week 4 Complete:** Overall coverage 70%+, documentation updated

---

## Resource Requirements

### Development Effort

- **Total Hours:** 160 hours (4 weeks × 40 hours/week)
- **Team Size:** 1 developer full-time
- **Duration:** 4 weeks (1 month)
- **Cost:** $0 (internal quality work)

### Tools & Infrastructure

- **Existing:**
  - pytest (test framework)
  - hypothesis (property-based testing)
  - coverage.py (coverage tracking)
  - unittest (test framework)

- **No New Tools Required:** All tooling in place from v3.5.2

### Validation Environment

- **CI/CD:** GitHub Actions (existing)
- **Coverage Reports:** HTML/JSON/XML generation
- **Security Scans:** Bandit, Safety, pip-audit (existing)
- **Architecture Checks:** test_architecture.py (existing)

---

## Communication Plan

### Internal Updates

- **Weekly progress reports** showing coverage improvements
- **Phase completion summaries** with metrics
- **Blocker escalation** if property tests reveal critical bugs

### Documentation Updates

- **CLAUDE.md:** Updated with v3.5.3 status
- **docs/project/STATUS.md:** Coverage metrics updated weekly
- **CHANGELOG.md:** Comprehensive v3.5.3 entry on completion

### Release Notes (v3.5.3)

**Title:** Testing Excellence - 70% Coverage Achieved

**Highlights:**
- ✅ Property-based tests operational (21,000+ scenarios)
- ✅ Coverage increased 52% → 70% (+35% improvement)
- ✅ 4 new test files created (1,130 LOC)
- ✅ CLI security testing comprehensive (17% → 70%)
- ✅ Zero security vulnerabilities maintained
- ✅ 100% test pass rate maintained

---

## Appendix A: Coverage Gap Details

### Module-by-Module Analysis

**cli.py (17.55% → 70% target):**

**Untested Functions:**
- `bounded_int_validator` (lines 260-267)
- `bounded_float_validator` (lines 277-285)
- `config_init` (lines 350-366)
- `config_show` (lines 397-431)
- `config_set` (lines 445-469)
- `config_get` (lines 474-497)
- `config_reset` (lines 537-590)
- `cache_stats` (lines 624-678)
- `cache_clear` (lines 709-747)
- `report` (lines 767-880)

**Error Paths:**
- KeyboardInterrupt handling
- PermissionError handling
- Invalid input validation
- Config file errors

**Total Untested:** ~1,100 LOC

---

**output_formatter.py (62.30% → 80% target):**

**Untested Functions:**
- CSV escaping edge cases
- Unicode handling
- Schema version validation
- Format detection from file extension

**Total Untested:** ~70 LOC

---

**config_manager.py (57.48% → 75% target):**

**Untested Paths:**
- Malformed config file recovery
- Type validation edge cases
- Config migration logic
- Permission errors during save

**Total Untested:** ~90 LOC

---

**cache_manager.py (58.70% → 75% target):**

**Untested Paths:**
- Database corruption recovery
- Disk full errors
- HMAC calculation edge cases
- Batch operation partial failures
- Concurrent access edge cases

**Total Untested:** ~230 LOC

---

**scanner.py (60.29% → 70% target):**

**Untested Paths:**
- Worker thread crashes
- Extension discovery failures
- API timeout handling
- Partial result handling

**Total Untested:** ~140 LOC

---

**extension_discovery.py (63.98% → 80% target):**

**Untested Paths:**
- Malformed package.json recovery
- Permission errors
- Symbolic link handling
- Missing required fields

**Total Untested:** ~60 LOC

---

**utils.py (63.74% → 75% target):**

**Untested Functions:**
- Non-security utility functions
- Version comparison logic
- Timestamp formatting
- String manipulation utilities

**Total Untested:** ~70 LOC

---

## Appendix B: Test File Specifications

### New Test Files

1. **tests/test_output_formatter.py** (~100 LOC)
   - JSON format validation
   - CSV format validation
   - Schema version tests
   - Error handling

2. **tests/test_config_manager.py** (~100 LOC)
   - Config parsing
   - Validation logic
   - Default handling
   - Error recovery

3. **tests/test_extension_discovery.py** (~80 LOC)
   - Discovery logic
   - package.json parsing
   - Error handling

4. **tests/test_utils.py** (~100 LOC)
   - Non-security utilities
   - Version handling
   - Format utilities

### Expanded Test Files

1. **tests/test_cli.py** (+500 LOC)
   - Input validators (100 LOC)
   - Config commands (200 LOC)
   - Cache commands (100 LOC)
   - Report commands (100 LOC)

2. **tests/test_cache_integrity.py** (+150 LOC)
   - Error recovery (50 LOC)
   - Expiration edge cases (50 LOC)
   - Batch operations (50 LOC)

3. **tests/test_scanner.py** (+100 LOC)
   - Error paths (100 LOC)

4. **tests/test_api.py** (+50 LOC)
   - Rare error paths (50 LOC)

5. **tests/test_display.py** (+50 LOC)
   - Edge cases (50 LOC)

---

## Appendix C: Property Test Reference

### Property-Based Testing Primer

**What is Property-Based Testing?**

Traditional example-based testing:
```python
def test_addition():
    assert add(2, 3) == 5  # One specific example
```

Property-based testing:
```python
@given(st.integers(), st.integers())
def test_addition_commutative(a, b):
    assert add(a, b) == add(b, a)  # Property: commutativity
    # Hypothesis generates 1000+ combinations of a, b
```

**Why Use Property-Based Tests?**

1. **Edge Case Discovery:** Automatically finds inputs that break assumptions
2. **Comprehensive Coverage:** Tests 1000+ scenarios per property
3. **Security Validation:** Proves invariants hold (e.g., "never crashes", "always blocks traversal")
4. **Regression Prevention:** Shrinks failures to minimal reproducible examples

**Example: Path Validation Property**

```python
@given(st.text())
@settings(max_examples=1000, deadline=None)
def test_validate_path_never_crashes(self, path_input):
    """PROPERTY: validate_path should handle any string without crashing."""
    try:
        result = validate_path(path_input)
        self.assertIsInstance(result, str)
    except (ValueError, OSError):
        pass  # Expected for invalid paths
    except Exception as e:
        self.fail(f"Unexpected crash: {e}")
```

**This single test generates 1000+ random strings including:**
- Empty strings
- Unicode characters
- Special characters
- Path traversal attempts
- Null bytes
- Extremely long strings
- Binary data

**Security Properties Tested:**
- "Traversal attempts are ALWAYS blocked" (zero false negatives)
- "System paths are ALWAYS blocked"
- "Null bytes are ALWAYS rejected"
- "Function NEVER crashes" (robustness)

---

## Document History

**Version:** 1.0
**Created:** 2025-10-29
**Author:** Claude Code (AI Assistant)
**Status:** 🎯 PLANNED

**Revisions:**
- v1.0 (2025-10-29): Initial roadmap creation

**References:**
- v3.5.2 roadmap: [docs/archive/plans/v3.5.2-roadmap.md](../archive/plans/v3.5.2-roadmap.md)
- Testing guide: [docs/guides/TESTING.md](../guides/TESTING.md)
- Coverage report: Generated via `coverage report`
