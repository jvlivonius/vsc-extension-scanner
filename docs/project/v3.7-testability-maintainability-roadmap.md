# v3.7 Testability & Maintainability Roadmap

**Version:** 3.7.0 (Planned)
**Status:** Planning Phase
**Created:** 2025-01-04
**Goal:** Improve test coverage efficiency and codebase maintainability through architectural refactoring

---

## Executive Summary

### Current State (v3.6.0)

- **Coverage:** 78.94% (exceeds 75% target by 5.3%)
- **Total Tests:** 831 tests (23,288 lines of test code)
- **Test Files:** 54 test files
- **Source Code:** ~11,500 lines across 14 modules
- **Test-to-Code Ratio:** 2:1 (exceptionally high)

### Problem Statement

The project has reached **natural coverage ceilings** due to framework integration complexity. Remaining coverage gaps are primarily in:

1. **CLI output complexity** (display.py, cli.py) - Unnecessary `--plain` mode adds testing burden
2. **ThreadPoolExecutor integration** (scanner.py) - Hard to unit test concurrent code
3. **Typer framework coupling** (cli.py) - Business logic mixed with framework
4. **Legacy migration code** (cache_manager.py) - 115 lines of low-value code
5. **Over-testing** - 48 retry tests covering similar scenarios

**Key Insight:** Further improvements require **architectural refactoring** to extract testable business logic and **simplification** to remove unnecessary complexity, not adding more integration tests.

### v3.7 Goals

**Coverage Targets:**
- **Overall:** 78.94% → 88-90% (+10-12%)
- **scanner.py:** 71.03% → 85%+ (+14%)
- **cli.py:** 67.55% → 80%+ (+13%)
- **cache_manager.py:** 71.10% → 88%+ (+17%)
- **display.py:** 94.90% → 96%+ (simpler code = better coverage)

**Test Efficiency:**
- **Total Tests:** 831 → ~730 (-12%, -101 tests)
- **Test LOC:** 23,288 → ~16,500 (-29%)
- **Test Files:** 54 → ~49 (-5 files)

**Code Reduction:**
- **Production Code:** -265 lines (plain mode + cache migration)
- **Test Code:** -6,788 lines (-29%)

**Quality Improvements:**
- ✅ Simpler CLI (remove `--plain` mode)
- ✅ More unit tests, fewer integration tests
- ✅ Better code organization with clear separation of concerns
- ✅ Faster development cycles (5s unit tests vs 40s full suite)
- ✅ Reduced complexity in core modules
- ✅ Eliminated legacy technical debt

### Strategy

**Four-Phase Approach:**

1. **Phase 0: CLI Simplification** (4-6 hours) - Remove plain mode, foundation for other improvements
2. **Phase 1: Foundation** (10 hours) - Quick wins, low risk
3. **Phase 2: Architecture** (22 hours) - Core refactoring
4. **Phase 3: Polish** (12 hours) - Optimization and tooling

**Total Effort:** ~52 hours (~6.5 days)

---

## Phase 0: CLI Output Simplification ⭐ DO FIRST

**Timeline:** 1 week
**Effort:** 4-6 hours
**Risk:** LOW (simplification, removes code)
**Dependencies:** None
**Breaking Change:** YES (removes `--plain` flag)

### Objective

Simplify CLI output system by removing unnecessary `--plain` mode:
- Remove `--plain` flag and all associated logic
- Simplify display.py (~150 lines removed)
- Keep only Rich (default) and Quiet (CI) modes
- Foundation for simpler CLI validation in Phase 2

### Rationale

**Current Complexity:**
- Three output modes: Rich, Plain, Quiet
- `display.py` has extensive plain mode branching (~150 lines)
- Tests must cover all three modes (~20 additional tests)
- Confusing distinction between "plain" and "quiet"

**User Requirements:**
- Interactive users: Always want Rich mode (colors, progress)
- CI users: Need minimal output (`--quiet` sufficient)
- Assumption: Modern CI systems support ANSI colors
- JSON/CSV provide detailed data for automated processing

**Benefits:**
- Simpler codebase (-150 lines production, -20 tests)
- Foundation for Phase 2.3 (CLI validation extraction)
- Clearer user intent (Rich vs Quiet, not Rich vs Plain vs Quiet)
- Reduced testing burden

---

### 0.1 Remove --plain Flag

**Priority:** CRITICAL
**Effort:** 1 hour
**Risk:** LOW (breaking change, requires documentation)

#### Changes Required

**File: vscode_scanner/cli.py**

```python
# REMOVE this parameter
@app.command()
def scan(
    # ... other parameters ...
    plain: bool = typer.Option(False, "--plain", help="Plain text output"),  # DELETE THIS
    quiet: bool = typer.Option(False, "--quiet", "-q", help="Minimal output for CI"),
):
    """Scan installed VS Code extensions"""

    # Remove all references to `plain` variable
    # Pass only `quiet` to display functions
```

**Documentation Changes:**
```markdown
# README.md, CHANGELOG.md

## CLI Flags

- `--quiet, -q` - Minimal output (errors + one-line summary) for CI/CD
- ~~`--plain`~~ - REMOVED in v3.7 (use `--quiet` instead)
```

---

### 0.2 Simplify display.py

**Priority:** CRITICAL
**Effort:** 2-3 hours
**Risk:** LOW (simplification)

#### Current Problem

[display.py](../../vscode_scanner/display.py) has extensive plain mode branching:

```python
def create_scan_progress(plain: bool = False) -> Progress:
    """Create progress display"""
    if plain:
        return None  # No progress in plain mode
    return Progress(...)  # Rich progress

def print_summary_table(results, plain=False, quiet=False):
    """Print scan summary"""
    if quiet:
        # Minimal output
    elif plain:
        # Plain text table (~50 lines)
    else:
        # Rich table

def _print_plain_summary(results):
    """Plain text summary - 50+ lines"""
    # Complex plain formatting logic
```

**Issues:**
- ~150 lines dedicated to plain mode
- Multiple `if plain:` branches throughout
- Separate plain formatting functions
- Tests must cover all branches

#### Proposed Simplification

**Remove all plain mode logic:**

```python
# display.py - SIMPLIFIED

def create_scan_progress() -> Progress:
    """Create Rich progress display (always Rich, no plain mode)"""
    return Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeRemainingColumn(),
    )

def print_summary_table(results: List[Dict], quiet: bool = False) -> None:
    """
    Print scan summary.

    Args:
        results: Scan results
        quiet: If True, print one-line summary; otherwise Rich table
    """
    if quiet:
        # One-line summary for CI
        total = len(results)
        high_risk = sum(1 for r in results if r.get('risk_level') == 'high')
        vulns = sum(r.get('vulnerabilities', {}).get('count', 0) for r in results)
        print(f"vscan: {total} extensions scanned, {high_risk} high risk, {vulns} vulnerabilities found")
        return

    # Rich table (existing logic, remove plain branches)
    table = Table(title="Scan Results", show_header=True)
    table.add_column("Extension", style="cyan")
    table.add_column("Risk", style="yellow")
    table.add_column("Vulnerabilities", style="red")

    for result in results:
        table.add_row(
            result['id'],
            result.get('risk_level', 'unknown').upper(),
            str(result.get('vulnerabilities', {}).get('count', 0))
        )

    console.print(table)

def print_scan_results(results, quiet=False):
    """Print detailed scan results"""
    if quiet:
        return  # No detailed output in quiet mode

    # Rich output only (remove plain mode branches)
    for result in results:
        # Rich panels and formatting
        ...

# DELETE _print_plain_summary() function entirely
```

**ProgressCallback Simplification:**

```python
class ProgressCallback:
    """Progress tracking for scan operations"""

    def __init__(self, total: int, quiet: bool = False):
        """
        Initialize progress callback.

        Args:
            total: Total number of extensions to scan
            quiet: If True, suppress progress output
        """
        self.total = total
        self.quiet = quiet
        self.completed = 0

        if not quiet:
            # Rich mode: Create progress display
            self.progress = create_scan_progress()
            self.task = self.progress.add_task("Scanning extensions...", total=total)
            self.live = Live(self.progress, console=console)
            self.live.start()
        else:
            # Quiet mode: No progress display
            self.progress = None
            self.live = None

    def update(self, completed: int) -> None:
        """Update progress"""
        self.completed = completed
        if not self.quiet and self.progress:
            self.progress.update(self.task, completed=completed)
        # Quiet mode: No output during scan

    def error(self, ext: Dict, error: str) -> None:
        """Report scan error"""
        if self.quiet:
            # Quiet mode: Show errors/warnings only
            console.print(f"[yellow]Warning:[/yellow] {ext['id']} - {error}", style="")
        elif self.progress:
            # Rich mode: Show error in progress context
            self.progress.console.print(f"[red]Error:[/red] {ext['id']} - {error}")

    def finish(self) -> None:
        """Finish progress tracking"""
        if not self.quiet and self.live:
            self.live.stop()
```

**Changes Summary:**
- Remove `plain` parameter from all functions
- Remove all `if plain:` branches
- Delete `_print_plain_summary()` and related plain formatting functions
- Simplify to binary choice: Rich or Quiet
- **Estimated LOC removed:** ~150 lines

---

### 0.3 Update Tests

**Priority:** HIGH
**Effort:** 1 hour
**Risk:** LOW (test cleanup)

#### Tests to Remove

**test_display.py:**
- `test_plain_mode_summary()` - DELETE
- `test_plain_mode_table()` - DELETE
- `test_plain_mode_no_progress()` - DELETE
- `test_plain_vs_rich_output()` - DELETE
- ~10 plain mode tests total

**test_cli.py:**
- `test_scan_with_plain_flag()` - DELETE
- `test_plain_flag_compatibility()` - DELETE
- ~5 plain flag tests total

**test_scanner_progress.py:**
- `test_progress_callback_plain_mode()` - DELETE
- `test_plain_mode_no_display()` - DELETE
- ~5 plain mode progress tests total

**Total Tests Removed:** ~20 tests

#### Tests to Add/Update

**test_display.py - Quiet Mode:**

```python
def test_quiet_mode_one_line_summary():
    """Test that quiet mode prints one-line summary"""
    results = [
        {'id': 'ext1', 'risk_level': 'low', 'vulnerabilities': {'count': 0}},
        {'id': 'ext2', 'risk_level': 'high', 'vulnerabilities': {'count': 3}},
    ]

    with patch('builtins.print') as mock_print:
        print_summary_table(results, quiet=True)

    mock_print.assert_called_once()
    output = mock_print.call_args[0][0]
    assert "2 extensions scanned" in output
    assert "1 high risk" in output
    assert "3 vulnerabilities found" in output

def test_quiet_mode_shows_errors():
    """Test that quiet mode displays errors/warnings"""
    callback = ProgressCallback(total=5, quiet=True)
    ext = {'id': 'test-ext'}

    with patch('rich.console.Console.print') as mock_print:
        callback.error(ext, "API Error")

    mock_print.assert_called_once()
    assert "Warning:" in str(mock_print.call_args)
    assert "test-ext" in str(mock_print.call_args)

def test_quiet_mode_suppresses_progress():
    """Test that quiet mode suppresses progress display"""
    callback = ProgressCallback(total=5, quiet=True)

    assert callback.progress is None
    assert callback.live is None

    # update() should not raise
    callback.update(3)  # Should do nothing
    callback.finish()   # Should do nothing

def test_rich_mode_shows_progress():
    """Test that Rich mode creates progress display"""
    callback = ProgressCallback(total=5, quiet=False)

    assert callback.progress is not None
    assert callback.live is not None
    assert callback.task is not None

    callback.finish()
```

**test_cli.py - Flag Removal:**

```python
def test_scan_quiet_flag():
    """Test that --quiet flag works correctly"""
    result = runner.invoke(app, ["scan", "--quiet"])

    # Should complete successfully
    assert result.exit_code in [0, 1]  # 0=no vulns, 1=vulns found

    # Output should be minimal (one line)
    lines = [line for line in result.output.split('\n') if line.strip()]
    assert len(lines) <= 3  # Summary + maybe warning/error

def test_scan_default_rich_mode():
    """Test that default mode uses Rich formatting"""
    result = runner.invoke(app, ["scan"])

    # Should have Rich output (tables, colors)
    # Rich uses ANSI escape codes
    assert '\033[' in result.output or 'Scanning' in result.output
```

---

### 0.4 Update Documentation

**Priority:** HIGH
**Effort:** 0.5-1 hour
**Risk:** LOW

#### Files to Update

**README.md:**

```markdown
## Usage

### Basic Scan
```bash
vscan scan
```
Rich output with progress bars and colored tables.

### CI/CD Mode
```bash
vscan scan --quiet
```
Minimal output for automated pipelines (errors + one-line summary).

### Generate Reports
```bash
vscan scan --output results.json
vscan scan --output results.csv
vscan scan --output report.html
```
```

**CHANGELOG.md:**

```markdown
## [3.7.0] - YYYY-MM-DD

### BREAKING CHANGES

#### CLI Output Simplification

**Removed `--plain` flag:**
- The `--plain` flag has been removed
- Interactive mode now always uses Rich formatting (colors, progress bars, tables)
- For CI/CD environments, use `--quiet` flag instead

**Migration:**
```bash
# Before (v3.6)
vscan scan --plain

# After (v3.7)
vscan scan --quiet
```

**Rationale:**
- Simplifies codebase (-150 lines in display.py)
- Removes unnecessary testing burden (-20 tests)
- Clearer user intent (Rich for interactive, Quiet for CI)
- Modern CI systems support ANSI colors
- JSON/CSV outputs provide detailed data for automated processing

**For CI/CD Users:**
The `--quiet` flag provides minimal console output:
- Suppresses progress indicators
- Shows errors and warnings
- Displays one-line summary at end
- Exit code indicates scan status (0=clean, 1=vulns, 2=error)

**For Interactive Users:**
No changes needed - Rich mode is now the default and only interactive mode.
```

**docs/guides/TESTING.md:**

Remove section on testing plain mode output:

```markdown
## Display Testing

### Output Modes

~~The scanner supports multiple output modes:~~
~~- **Rich mode** (default) - Colorful output with progress bars~~
~~- **Plain mode** (`--plain`) - Simple text output~~
~~- **Quiet mode** (`--quiet`) - Minimal output~~

The scanner supports two output modes:
- **Rich mode** (default) - Colorful output with progress bars and tables
- **Quiet mode** (`--quiet`) - Minimal output (errors + one-line summary)

### Testing Rich Output

Use Rich's `Console` capture for testing:
```python
from io import StringIO
from rich.console import Console

def test_rich_output():
    console = Console(file=StringIO())
    print_summary_table(results, quiet=False)
    output = console.file.getvalue()
    # Assert on captured output
```

### Testing Quiet Output

Test quiet mode directly:
```python
def test_quiet_output():
    with patch('builtins.print') as mock_print:
        print_summary_table(results, quiet=True)

    # Verify one-line summary
    assert mock_print.call_count == 1
```
```

---

### 0.5 Migration & Communication

**Priority:** MEDIUM
**Effort:** 0.5 hour
**Risk:** LOW

#### Communication Plan

**GitHub Release Notes (v3.7.0):**

```markdown
## Breaking Changes

### CLI Output Simplification

The `--plain` flag has been **removed** in v3.7.0.

**Why?**
- Simplified codebase (removed 150+ lines of plain mode logic)
- Reduced testing complexity (removed 20 plain mode tests)
- Clearer user intent: Rich (interactive) vs Quiet (CI)
- Modern assumption: CI systems support ANSI colors

**Migration:**
If you were using `--plain` in CI/CD:
```bash
# Replace this
vscan scan --plain

# With this
vscan scan --quiet
```

**What `--quiet` provides:**
✅ Suppresses progress indicators
✅ Shows errors and warnings
✅ One-line summary at end
✅ Exit code indicates status (0/1/2)
✅ Perfect for CI/CD pipelines

**For detailed data:**
Use output files instead of console parsing:
```bash
vscan scan --quiet --output results.json
# Parse JSON for detailed information
```

**For interactive use:**
No changes needed - Rich mode remains the excellent default experience.
```

**Issue Template Update:**

```markdown
## Bug Report Template

**Environment:**
- OS: [e.g., Ubuntu 22.04]
- vscan version: [e.g., 3.7.0]
- Python version: [e.g., 3.11.5]
- CLI flags used: [e.g., --quiet, --output results.json]
~~- Output mode: [Rich/Plain/Quiet]~~ ← Remove this (only Rich/Quiet now)
```

---

### Phase 0 Outcomes

**After Phase 0 Completion:**

✅ **Code Reduction:**
- display.py: -150 lines (plain mode logic removed)
- cli.py: -10 lines (plain flag removed)
- Total production code: -160 lines

✅ **Test Reduction:**
- Removed plain mode tests: ~20 tests
- Simplified test cases: ~10 tests cleaner
- Net: 831 → 811 tests (-20 tests)

✅ **Coverage Improvements:**
- display.py: 94.90% → 96%+ (fewer branches)
- Simpler code = easier to test

✅ **Maintainability:**
- Clear binary choice: Rich or Quiet
- No confusing plain vs quiet distinction
- Foundation for Phase 2.3 (CLI validation)

✅ **Documentation:**
- Clear migration path documented
- Breaking change communicated
- Updated examples throughout

**Risk Assessment:** ✅ LOW
- Breaking change is clearly documented
- Migration is trivial (`--plain` → `--quiet`)
- Improved user experience overall

---

## Phase 1: Foundation (Quick Wins)

**Timeline:** 1-2 weeks
**Effort:** 10 hours (REDUCED from 12 hours)
**Risk:** Low
**Dependencies:** Phase 0 complete

### Objective

Establish foundation for testability improvements through:
- Simplified cache management
- Test organization improvements
- Shared test infrastructure

### 1.1 Simplify Cache Schema Management ⭐ BREAKING CHANGE

**Priority:** CRITICAL
**Effort:** 2-3 hours
**Risk:** Low (breaking change, requires documentation)

#### Current Problem

[cache_manager.py:532-646](../../vscode_scanner/cache_manager.py#L532-L646) contains 115 lines of v1→v2 migration code:

```python
def _migrate_v1_to_v2(self):
    """Migrate cache database from v1 to v2 schema"""
    # 115 lines of complex migration logic
    # - Schema detection
    # - Data transformation
    # - HMAC signature generation
    # - Transaction management
    # - Error handling
```

**Problems:**
- 82% of cache_manager.py coverage gap
- Complex code that requires extensive testing
- Low value (v1.0 released Oct 2024, migration window closed)
- Maintenance burden for legacy schema

#### New Approach: Auto-Regenerate on Schema Changes

**Philosophy:** Cache is **ephemeral data** - it can always be regenerated from source.

**Implementation:**

```python
# In cache_manager.py __init__

CURRENT_SCHEMA_VERSION = "3.0"  # Bump to 3.0 for v3.7

def __init__(self, cache_dir: Optional[str] = None):
    self.cache_db = Path(cache_dir or self._get_default_cache_dir()) / "cache.db"
    self._init_cache_dir()

    # Check for schema version mismatch
    if self.cache_db.exists():
        existing_version = self._get_schema_version()

        if existing_version != CURRENT_SCHEMA_VERSION:
            logger.info(
                f"Cache schema version {existing_version} detected. "
                f"Current version is {CURRENT_SCHEMA_VERSION}. "
                f"Removing old cache and creating new schema..."
            )
            self.cache_db.unlink()  # Remove old database

    # Initialize with current schema
    self._init_db()

def _get_schema_version(self) -> str:
    """Get schema version from existing database"""
    try:
        conn = sqlite3.connect(self.cache_db)
        cursor = conn.cursor()
        cursor.execute("SELECT version FROM schema_version LIMIT 1")
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else "unknown"
    except sqlite3.OperationalError:
        return "unknown"
```

**User Experience:**

```bash
$ vscan scan
Cache schema version 2.0 detected. Current version is 3.0.
Removing old cache and creating new schema...
Cache regenerated. Scanning 42 extensions...
```

**Changes Required:**

1. **Remove migration code** (lines 532-646)
2. **Simplify __init__** with version check + auto-remove
3. **Bump CURRENT_SCHEMA_VERSION** to 3.0
4. **Update documentation** - explain cache regeneration approach
5. **Add user messaging** - log when cache is regenerated

**Testing:**

- Add test for schema version mismatch detection
- Add test for automatic cache removal
- Add test for fresh cache creation after removal
- **Remove all migration tests** (~15 test cases)

**Documentation:**

- Update [CHANGELOG.md](../../CHANGELOG.md) - Breaking change section
- Update [README.md](../../README.md) - Cache management explanation
- Update [docs/guides/ARCHITECTURE.md](../guides/ARCHITECTURE.md) - Cache design philosophy

**Impact:**
- **Coverage:** cache_manager.py 71.10% → 88%+ (+17%)
- **Code Reduction:** -115 lines
- **Test Reduction:** -15 migration tests
- **Maintenance:** Eliminate legacy code burden

---

### 1.2 Consolidate Scanner Tests

**Priority:** HIGH
**Effort:** 3-4 hours (REDUCED - Phase 0 already removed plain mode tests)
**Risk:** None (no code changes)

#### Benefits from Phase 0

**Phase 0 Simplifications:**
- Plain mode tests already removed from test_scanner_progress.py
- Progress callback tests now only cover Rich/Quiet modes
- Simpler test organization (no 3-way mode testing)

#### Current Problem

Scanner has **6 separate test files** with overlapping concerns:

| File | Lines | Tests | Focus |
|------|-------|-------|-------|
| test_scanner.py | 1,042 | 56 | Main workflow |
| test_scanner_filters.py | 583 | 28 | Filter logic |
| test_scanner_progress.py | ~400 | ~11 | Progress callbacks (after Phase 0) |
| test_scanner_error_recovery.py | 199 | 15 | Error handling |
| test_scanner_edge_cases.py | 240 | 6 | Edge cases |
| test_scanner_output_errors.py | 95 | 2 | Output errors |
| **Total** | **~2,560** | **~118** | (after Phase 0 cleanup) |

**Problems:**
- Duplicate setup/teardown code across files
- Difficult to find related tests
- No clear organization principle
- High maintenance burden

#### Proposed Organization

**Consolidate into 3 focused files:**

```
tests/
├── test_scanner_core.py          # Main scanning workflow (56 tests)
│   ├── Discovery and initialization
│   ├── Cache integration
│   ├── API interaction
│   ├── Output file generation
│   └── Exit code logic
│
├── test_scanner_filters.py       # All filtering logic (28 tests)
│   ├── Publisher filtering
│   ├── Risk level filtering
│   ├── Verified publisher filtering
│   └── Vulnerability filtering
│
└── test_scanner_integration.py   # Integration scenarios (36 tests)
    ├── Progress callback integration (13 tests)
    ├── Error recovery and categorization (15 tests)
    ├── Edge cases and boundary conditions (6 tests)
    └── Output error handling (2 tests)
```

**Implementation Plan:**

1. **Create shared fixture file** (`tests/fixtures/scanner_fixtures.py`)
   - Common mock setups
   - Sample extension data
   - API response fixtures
   - Progress callback mocks

2. **Merge test files** with clear section comments
   ```python
   # test_scanner_core.py

   # ============================================================================
   # Discovery and Initialization Tests
   # ============================================================================

   def test_discover_extensions_success():
       ...

   # ============================================================================
   # Cache Integration Tests
   # ============================================================================

   def test_uses_cache_when_available():
       ...
   ```

3. **Update imports** in consolidated files
4. **Verify all 120 tests** still run successfully

**Benefits:**
- Easier to find related tests
- Shared setup/teardown reduces duplication
- Clear organization by concern
- **Estimated LOC reduction:** -15% (from 2,582 to ~2,200)

**Testing:**
- Run full test suite before consolidation
- Run full test suite after consolidation
- Verify identical coverage reports

---

### 1.3 Create Shared Test Fixtures

**Priority:** HIGH
**Effort:** 3-4 hours
**Risk:** None (refactoring only)

#### Current Problem

**Extensive duplication** across 54 test files:

- Extension metadata recreated in 20+ files
- API response mocks duplicated in 15+ files
- Mock configuration repeated throughout
- Inconsistent test data across files

**Example of Duplication:**

```python
# Found in test_scanner.py, test_cache_manager.py, test_display.py, etc.
sample_ext = {
    "id": "ms-python.python",
    "version": "2024.1.0",
    "publisher": "ms-python",
    "name": "python",
    "display_name": "Python",
}

sample_result = {
    "scan_status": "success",
    "risk_level": "low",
    "vulnerabilities": {"count": 0},
}
```

#### Proposed Solution

**Create canonical fixtures module:**

```python
# tests/fixtures/canonical_fixtures.py

"""
Canonical test fixtures for vsc-extension-scanner test suite.

This module provides a single source of truth for test data used across
multiple test files. All fixtures are based on real extension data but
simplified for testing purposes.
"""

import pytest
from datetime import datetime
from pathlib import Path
from typing import Dict, List
from unittest.mock import Mock

# ============================================================================
# Extension Data Fixtures
# ============================================================================

@pytest.fixture
def sample_extension() -> Dict:
    """Sample VS Code extension metadata"""
    return {
        "id": "ms-python.python",
        "version": "2024.1.0",
        "publisher": "ms-python",
        "name": "python",
        "display_name": "Python",
        "publisher_domain": "microsoft.com",
        "verified": True,
        "install_count": 50000000,
        "last_updated": "2024-01-01T00:00:00Z",
    }

@pytest.fixture
def sample_extension_list() -> List[Dict]:
    """List of sample extensions covering different scenarios"""
    return [
        # Verified publisher, no vulnerabilities
        {
            "id": "ms-python.python",
            "publisher": "ms-python",
            "name": "python",
            "display_name": "Python",
            "version": "2024.1.0",
            "verified": True,
        },
        # Unverified publisher, has vulnerabilities
        {
            "id": "test-publisher.test-extension",
            "publisher": "test-publisher",
            "name": "test-extension",
            "display_name": "Test Extension",
            "version": "1.0.0",
            "verified": False,
        },
        # High risk extension
        {
            "id": "risky.extension",
            "publisher": "risky",
            "name": "extension",
            "display_name": "Risky Extension",
            "version": "0.1.0",
            "verified": False,
        },
    ]

# ============================================================================
# API Response Fixtures
# ============================================================================

@pytest.fixture
def sample_scan_result_success() -> Dict:
    """Successful scan result with no vulnerabilities"""
    return {
        "scan_status": "success",
        "risk_level": "low",
        "vulnerabilities": {
            "count": 0,
            "critical": 0,
            "high": 0,
            "medium": 0,
            "low": 0,
        },
        "analysis_id": "test-analysis-123",
        "scanned_at": datetime.utcnow().isoformat(),
    }

@pytest.fixture
def sample_scan_result_with_vulns() -> Dict:
    """Scan result with vulnerabilities"""
    return {
        "scan_status": "success",
        "risk_level": "high",
        "vulnerabilities": {
            "count": 3,
            "critical": 1,
            "high": 2,
            "medium": 0,
            "low": 0,
            "details": [
                {
                    "id": "VULN-001",
                    "severity": "critical",
                    "description": "Remote code execution vulnerability",
                },
                {
                    "id": "VULN-002",
                    "severity": "high",
                    "description": "Unauthorized data access",
                },
                {
                    "id": "VULN-003",
                    "severity": "high",
                    "description": "Insecure dependency",
                },
            ],
        },
        "analysis_id": "test-analysis-456",
        "scanned_at": datetime.utcnow().isoformat(),
    }

@pytest.fixture
def sample_scan_result_error() -> Dict:
    """Error scan result"""
    return {
        "scan_status": "error",
        "error_type": "analysis_failed",
        "error_message": "Failed to analyze extension",
        "analysis_id": None,
    }

# ============================================================================
# Mock API Client Fixtures
# ============================================================================

@pytest.fixture
def mock_api_client(mocker, sample_scan_result_success):
    """Pre-configured mock API client with realistic behavior"""
    client = mocker.Mock()
    client.scan_extension.return_value = sample_scan_result_success
    client.delay = 1.5
    client.timeout = 30
    return client

@pytest.fixture
def mock_api_client_with_retries(mocker, sample_scan_result_success):
    """Mock API client that succeeds after retries"""
    client = mocker.Mock()
    # First two calls fail, third succeeds
    client.scan_extension.side_effect = [
        Exception("Temporary error"),
        Exception("Temporary error"),
        sample_scan_result_success,
    ]
    return client

# ============================================================================
# Cache Fixtures
# ============================================================================

@pytest.fixture
def temp_cache_dir(tmp_path) -> Path:
    """Temporary cache directory for testing"""
    cache_dir = tmp_path / ".vscan"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir

@pytest.fixture
def mock_cache_manager(mocker, sample_scan_result_success):
    """Pre-configured mock cache manager"""
    cache = mocker.Mock()
    cache.get.return_value = None  # Miss by default
    cache.save.return_value = None
    cache.get_stats.return_value = {"hits": 0, "misses": 0, "total": 0}
    return cache

# ============================================================================
# Progress Display Fixtures
# ============================================================================

@pytest.fixture
def mock_progress_callback(mocker):
    """Mock progress callback for testing"""
    callback = mocker.Mock()
    callback.started.return_value = None
    callback.completed.return_value = None
    callback.scanned.return_value = None
    callback.cache_hit.return_value = None
    callback.error.return_value = None
    callback.finish.return_value = None
    return callback

# ============================================================================
# Config Fixtures
# ============================================================================

@pytest.fixture
def mock_config_manager(mocker):
    """Mock configuration manager with default settings"""
    config = mocker.Mock()
    config.get.side_effect = lambda key, default=None: {
        "scan.workers": 3,
        "scan.delay": 1.5,
        "scan.timeout": 30,
        "cache.enabled": True,
        "cache.ttl": 86400,
    }.get(key, default)
    return config
```

**Implementation Plan:**

1. **Create fixtures module** with comprehensive documentation
2. **Identify duplicate test data** across existing test files
3. **Update test files** to use shared fixtures:
   ```python
   # Before
   def test_scan_extension():
       ext = {"id": "ms-python.python", ...}  # Duplicated
       result = {"scan_status": "success", ...}  # Duplicated

   # After
   def test_scan_extension(sample_extension, sample_scan_result_success):
       # Use fixtures directly
   ```
4. **Verify tests** pass with shared fixtures

**Benefits:**
- **Single source of truth** for test data
- **Consistency** across all tests
- **Easy maintenance** - update once, apply everywhere
- **Discoverability** - clear fixture catalog
- **Estimated LOC reduction:** -10% through eliminated duplication

---

### Phase 1 Outcomes

**After Phase 1 Completion:**

✅ **Coverage Improvements:**
- cache_manager.py: 71.10% → 88%+ (+17%)
- Overall: 78.94% → 82%+ (+3-4%)

✅ **Code Reduction:**
- Production code: -115 lines (cache_manager.py)
- Test code: -15% in scanner tests (~400 lines)
- Test code: -10% through fixtures (~2,300 lines)

✅ **Test Efficiency:**
- Consolidated scanner tests: 6 files → 3 files
- Shared fixtures reduce duplication
- Removed migration tests (~15 tests)

✅ **Maintainability:**
- Simpler cache management (no legacy code)
- Better test organization
- Consistent test data

**Risk Assessment:** ✅ LOW
- Cache regeneration is user-friendly
- Test reorganization is safe refactoring
- Shared fixtures improve consistency

---

## Phase 2: Architecture (Core Refactoring)

**Timeline:** 3-4 weeks
**Effort:** 24 hours
**Risk:** Medium
**Dependencies:** Phase 1 complete

### Objective

Improve core module testability through architectural refactoring:
- Extract business logic from framework integration
- Reduce coupling in scanner orchestration
- Simplify retry testing with property-based approach
- Separate CLI validation from framework

### 2.1 Extract ScanOrchestrator Pattern ⭐ HIGHEST IMPACT

**Priority:** CRITICAL
**Effort:** 8-12 hours
**Risk:** Medium (architectural change)

#### Current Problem

[scanner.py:502-649](../../vscode_scanner/scanner.py#L502-L649) contains **147 lines** mixing multiple concerns:

```python
def _scan_extensions(
    extensions: List[Dict],
    args,
    cache_manager,
    config,
    on_progress=None,
    stats=None
) -> List[Dict]:
    """
    Scan extensions in parallel using ThreadPoolExecutor.

    COMPLEXITY ISSUES:
    - ThreadPoolExecutor orchestration (lines 502-520)
    - Progress callback integration (lines 521-540)
    - Thread-safe stats collection (lines 541-560)
    - Worker result processing (lines 561-590)
    - Error categorization (lines 591-620)
    - Cache persistence (lines 621-640)
    - Final stats reporting (lines 641-649)
    """

    # Setup
    max_workers = min(args.workers, len(extensions))
    results = []

    # ThreadPoolExecutor integration - HARD TO UNIT TEST
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        # Submit work
        for idx, ext in enumerate(extensions, 1):
            future = executor.submit(
                _scan_single_extension_worker,
                ext,
                args,
                cache_manager,
                config,
                idx,
                len(extensions)
            )
            futures[future] = (ext, idx)

        # Collect results - NON-DETERMINISTIC ORDER
        for future in as_completed(futures):
            ext, idx = futures[future]

            try:
                result = future.result()
                results.append(result)

                # Update progress - SIDE EFFECT
                if on_progress:
                    on_progress.completed(ext, result)

                # Update stats - THREAD-SAFE OPERATION
                if stats:
                    stats.increment(result['status'])

            except Exception as e:
                # Error handling - COMPLEX CATEGORIZATION
                error_type = _categorize_error(e)
                results.append({"error": error_type, ...})

                if on_progress:
                    on_progress.error(ext, error_type)

    # Cache persistence - SIDE EFFECT
    cache_manager.commit_batch()

    return results
```

**Why It's Hard to Test:**

1. **ThreadPoolExecutor behavior is non-deterministic**
   - `as_completed()` order varies
   - Race conditions difficult to reproduce
   - Requires actual concurrency to validate thread safety

2. **Multiple side effects**
   - Progress callbacks (Rich UI integration)
   - Stats updates (thread-safe state)
   - Cache persistence (database I/O)
   - Error logging

3. **Integration dependencies**
   - Requires real ThreadPoolExecutor
   - Needs Rich Progress object
   - Database writes in main thread
   - Worker function coupling

**Current Test Approach:**
- Integration tests with real ThreadPoolExecutor (23 tests)
- Mock Progress callbacks
- Real database for cache
- **Cannot unit test orchestration logic**

#### Proposed Refactoring: Separate Concerns

**Architecture:** Apply **Single Responsibility Principle** + **Dependency Inversion**

```
Current (Monolithic):
┌─────────────────────────────────────┐
│     _scan_extensions()              │
│  - ThreadPoolExecutor management    │
│  - Business logic (cache/API)       │
│  - Progress updates                 │
│  - Stats collection                 │
│  - Error handling                   │
│  - Cache persistence                │
└─────────────────────────────────────┘
        ↓
      HARD TO TEST

New (Separated):
┌─────────────────────┐     ┌──────────────────────┐
│  ScanOrchestrator   │────▶│  CacheInterface      │
│  (Pure Logic)       │     │  (Abstract)          │
│  - Check cache      │     └──────────────────────┘
│  - Call API         │
│  - Save to cache    │     ┌──────────────────────┐
│  - Return result    │────▶│  APIClientFactory    │
└─────────────────────┘     │  (Abstract)          │
        ↓                    └──────────────────────┘
    EASY TO UNIT TEST

┌─────────────────────┐     ┌──────────────────────┐
│  ParallelExecutor   │────▶│  ProgressDisplay     │
│  (Generic)          │     │  (Protocol)          │
│  - ThreadPool mgmt  │     └──────────────────────┘
│  - Progress updates │
│  - Error handling   │     ┌──────────────────────┐
│  - Generic worker   │────▶│  Worker Function     │
└─────────────────────┘     │  (Callable)          │
        ↓                    └──────────────────────┘
    TEST ONCE, REUSE
```

#### Implementation

**Step 1: Define Abstract Interfaces**

Create `vscode_scanner/protocols.py`:

```python
"""Protocol definitions for dependency injection and testing."""

from typing import Dict, Optional, Protocol, Callable
from datetime import datetime

class CacheInterface(Protocol):
    """Abstract cache interface for dependency injection"""

    def get(self, ext_id: str, version: str) -> Optional[Dict]:
        """Retrieve cached scan result"""
        ...

    def save(self, ext_id: str, version: str, result: Dict) -> None:
        """Save scan result to cache"""
        ...

    def get_stats(self) -> Dict:
        """Get cache statistics"""
        ...

class APIClientFactory(Protocol):
    """Factory for creating API client instances"""

    def __call__(self) -> "APIClientInterface":
        """Create new API client instance"""
        ...

class APIClientInterface(Protocol):
    """Abstract API client interface"""

    def scan_extension(self, publisher: str, name: str) -> Dict:
        """Scan extension via API"""
        ...

class ProgressDisplay(Protocol):
    """Abstract progress display interface"""

    def started(self, total: int) -> None:
        """Signal scan started"""
        ...

    def completed(self, ext: Dict, result: Dict) -> None:
        """Signal extension scan completed"""
        ...

    def cache_hit(self, ext: Dict) -> None:
        """Signal cache hit"""
        ...

    def error(self, ext: Dict, error: Exception) -> None:
        """Signal scan error"""
        ...

    def finish(self, stats: Dict) -> None:
        """Signal scan finished"""
        ...
```

**Step 2: Create Pure Business Logic**

Create `vscode_scanner/scan_orchestrator.py`:

```python
"""Pure business logic for extension scanning - no I/O, no threading."""

from typing import Dict, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

from .protocols import CacheInterface, APIClientFactory
from .utils import sanitize_string

logger = logging.getLogger(__name__)

@dataclass
class ScanResult:
    """Result of scanning a single extension"""
    extension_id: str
    version: str
    status: str  # 'success', 'error', 'cache_hit'
    result: Dict
    source: str  # 'cache', 'api'
    scanned_at: datetime
    error: Optional[Exception] = None

class ScanOrchestrator:
    """
    Pure business logic for extension scanning.

    This class contains NO:
    - Threading logic
    - Progress display
    - Direct I/O
    - Side effects

    All dependencies are injected via protocols, making this
    trivially testable with pure unit tests.
    """

    def __init__(
        self,
        cache: CacheInterface,
        api_factory: APIClientFactory,
        cache_enabled: bool = True
    ):
        """
        Initialize orchestrator with dependencies.

        Args:
            cache: Cache interface for storing/retrieving results
            api_factory: Factory for creating API client instances
            cache_enabled: Whether to use cache (for testing)
        """
        self.cache = cache
        self.api_factory = api_factory
        self.cache_enabled = cache_enabled

    def scan_extension(self, ext: Dict) -> ScanResult:
        """
        Scan a single extension.

        This is a PURE function (given the same inputs, always returns
        the same output). All side effects (cache reads/writes, API calls)
        are performed through injected dependencies.

        Args:
            ext: Extension metadata dict

        Returns:
            ScanResult with outcome
        """
        ext_id = ext['id']
        version = ext['version']
        scanned_at = datetime.utcnow()

        # Step 1: Check cache
        if self.cache_enabled:
            cached_result = self.cache.get(ext_id, version)
            if cached_result:
                logger.debug(f"Cache hit for {ext_id}@{version}")
                return ScanResult(
                    extension_id=ext_id,
                    version=version,
                    status='cache_hit',
                    result=cached_result,
                    source='cache',
                    scanned_at=scanned_at
                )

        # Step 2: Scan via API
        try:
            api_client = self.api_factory()
            result = api_client.scan_extension(
                ext['publisher'],
                ext['name']
            )

            # Step 3: Save to cache
            if self.cache_enabled:
                self.cache.save(ext_id, version, result)

            logger.debug(f"API scan successful for {ext_id}@{version}")
            return ScanResult(
                extension_id=ext_id,
                version=version,
                status='success',
                result=result,
                source='api',
                scanned_at=scanned_at
            )

        except Exception as e:
            logger.error(f"Scan failed for {ext_id}@{version}: {e}")
            return ScanResult(
                extension_id=ext_id,
                version=version,
                status='error',
                result={},
                source='api',
                scanned_at=scanned_at,
                error=e
            )
```

**Step 3: Generic Parallel Executor**

Create `vscode_scanner/parallel_executor.py`:

```python
"""Generic parallel execution with progress tracking."""

from typing import List, Callable, TypeVar, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

from .protocols import ProgressDisplay

logger = logging.getLogger(__name__)

T = TypeVar('T')  # Input type
R = TypeVar('R')  # Result type

class ParallelExecutor:
    """
    Generic parallel execution coordinator.

    Handles threading concerns separately from business logic.
    Can be tested once and reused for any parallel operation.
    """

    def __init__(self, max_workers: int = 3):
        """
        Initialize executor.

        Args:
            max_workers: Maximum concurrent workers (1-5)
        """
        if not 1 <= max_workers <= 5:
            raise ValueError("max_workers must be between 1 and 5")
        self.max_workers = max_workers

    def execute_parallel(
        self,
        items: List[T],
        worker_fn: Callable[[T], R],
        progress: Optional[ProgressDisplay] = None
    ) -> List[R]:
        """
        Execute worker function on items in parallel.

        Args:
            items: Items to process
            worker_fn: Pure function to apply to each item
            progress: Optional progress display

        Returns:
            List of results in completion order
        """
        if not items:
            return []

        # Adjust workers based on item count
        num_workers = min(self.max_workers, len(items))
        results = []

        # Signal start
        if progress:
            progress.started(len(items))

        # Execute in parallel
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            # Submit all work
            futures = {executor.submit(worker_fn, item): item
                      for item in items}

            # Collect results
            for future in as_completed(futures):
                item = futures[future]

                try:
                    result = future.result()
                    results.append(result)

                    # Update progress
                    if progress:
                        progress.completed(item, result)

                except Exception as e:
                    logger.error(f"Worker failed for {item}: {e}")

                    # Report error
                    if progress:
                        progress.error(item, e)

        # Signal finish
        if progress:
            progress.finish({
                'total': len(items),
                'completed': len(results),
                'errors': len(items) - len(results)
            })

        return results
```

**Step 4: Compose in scanner.py**

Update `scanner.py`:

```python
# In scanner.py

from .scan_orchestrator import ScanOrchestrator, ScanResult
from .parallel_executor import ParallelExecutor
from .protocols import CacheInterface, APIClientFactory

def _scan_extensions(
    extensions: List[Dict],
    args,
    cache_manager: CacheInterface,
    config,
    on_progress=None,
    stats=None
) -> List[Dict]:
    """
    Scan extensions in parallel.

    Simplified orchestration:
    1. Create orchestrator with injected dependencies
    2. Create parallel executor
    3. Execute with progress tracking
    4. Convert results to legacy format
    """

    # Create API client factory
    def api_factory():
        from .vscan_api import VscanAPIClient
        return VscanAPIClient(
            delay=args.delay,
            timeout=args.timeout,
            max_retries=config.get('scan.max_retries', 3)
        )

    # Create orchestrator (PURE BUSINESS LOGIC)
    orchestrator = ScanOrchestrator(
        cache=cache_manager,
        api_factory=api_factory,
        cache_enabled=not args.no_cache
    )

    # Create executor (GENERIC THREADING)
    executor = ParallelExecutor(max_workers=args.workers)

    # Execute in parallel
    scan_results: List[ScanResult] = executor.execute_parallel(
        items=extensions,
        worker_fn=orchestrator.scan_extension,
        progress=on_progress
    )

    # Convert to legacy format (for backward compatibility)
    results = []
    for scan_result in scan_results:
        results.append({
            'extension_id': scan_result.extension_id,
            'version': scan_result.version,
            'status': scan_result.status,
            'result': scan_result.result,
            'source': scan_result.source,
            'scanned_at': scan_result.scanned_at.isoformat(),
            'error': str(scan_result.error) if scan_result.error else None
        })

    return results
```

#### Testing Strategy

**New Unit Tests (scanner_orchestrator):**

```python
# tests/test_scan_orchestrator.py

"""Unit tests for ScanOrchestrator - pure business logic"""

import pytest
from datetime import datetime
from vscode_scanner.scan_orchestrator import ScanOrchestrator, ScanResult

def test_cache_hit_returns_cached_result():
    """Test that cache hit returns result from cache"""
    # Arrange
    mock_cache = Mock()
    mock_cache.get.return_value = {'scan_status': 'success'}
    mock_api_factory = Mock()

    orchestrator = ScanOrchestrator(
        cache=mock_cache,
        api_factory=mock_api_factory,
        cache_enabled=True
    )

    ext = {'id': 'test.ext', 'version': '1.0.0'}

    # Act
    result = orchestrator.scan_extension(ext)

    # Assert
    assert result.status == 'cache_hit'
    assert result.source == 'cache'
    assert mock_cache.get.called
    assert not mock_api_factory.called  # API not called on cache hit

def test_cache_miss_calls_api():
    """Test that cache miss triggers API call"""
    # Arrange
    mock_cache = Mock()
    mock_cache.get.return_value = None  # Cache miss

    mock_api_client = Mock()
    mock_api_client.scan_extension.return_value = {'scan_status': 'success'}

    mock_api_factory = Mock(return_value=mock_api_client)

    orchestrator = ScanOrchestrator(
        cache=mock_cache,
        api_factory=mock_api_factory
    )

    ext = {'id': 'test.ext', 'version': '1.0.0', 'publisher': 'test', 'name': 'ext'}

    # Act
    result = orchestrator.scan_extension(ext)

    # Assert
    assert result.status == 'success'
    assert result.source == 'api'
    assert mock_api_client.scan_extension.called
    assert mock_cache.save.called  # Result saved to cache

def test_api_failure_returns_error_result():
    """Test that API failures are handled gracefully"""
    # Arrange
    mock_cache = Mock()
    mock_cache.get.return_value = None

    mock_api_client = Mock()
    mock_api_client.scan_extension.side_effect = Exception("API Error")

    mock_api_factory = Mock(return_value=mock_api_client)

    orchestrator = ScanOrchestrator(
        cache=mock_cache,
        api_factory=mock_api_factory
    )

    ext = {'id': 'test.ext', 'version': '1.0.0', 'publisher': 'test', 'name': 'ext'}

    # Act
    result = orchestrator.scan_extension(ext)

    # Assert
    assert result.status == 'error'
    assert result.error is not None
    assert not mock_cache.save.called  # Don't cache errors

def test_cache_disabled_skips_cache():
    """Test that cache can be disabled"""
    # Arrange
    mock_cache = Mock()
    mock_api_client = Mock()
    mock_api_client.scan_extension.return_value = {'scan_status': 'success'}
    mock_api_factory = Mock(return_value=mock_api_client)

    orchestrator = ScanOrchestrator(
        cache=mock_cache,
        api_factory=mock_api_factory,
        cache_enabled=False  # Disabled
    )

    ext = {'id': 'test.ext', 'version': '1.0.0', 'publisher': 'test', 'name': 'ext'}

    # Act
    result = orchestrator.scan_extension(ext)

    # Assert
    assert result.status == 'success'
    assert not mock_cache.get.called  # Cache not checked
    assert not mock_cache.save.called  # Result not saved
```

**New Unit Tests (parallel_executor):**

```python
# tests/test_parallel_executor.py

"""Unit tests for ParallelExecutor - generic threading"""

import pytest
from time import sleep
from vscode_scanner.parallel_executor import ParallelExecutor

def test_executes_all_items():
    """Test that all items are processed"""
    # Arrange
    executor = ParallelExecutor(max_workers=3)
    items = [1, 2, 3, 4, 5]

    def square(x):
        return x * x

    # Act
    results = executor.execute_parallel(items, square)

    # Assert
    assert sorted(results) == [1, 4, 9, 16, 25]

def test_respects_max_workers():
    """Test that worker count is respected"""
    # Arrange
    executor = ParallelExecutor(max_workers=2)
    items = list(range(10))

    def slow_fn(x):
        sleep(0.01)
        return x

    # Act
    results = executor.execute_parallel(items, slow_fn)

    # Assert
    assert len(results) == 10

def test_handles_worker_errors():
    """Test that errors in workers are handled"""
    # Arrange
    executor = ParallelExecutor(max_workers=3)
    items = [1, 2, 3]

    def failing_fn(x):
        if x == 2:
            raise ValueError("Intentional error")
        return x

    # Act - should not raise
    results = executor.execute_parallel(items, failing_fn)

    # Assert - successful items processed
    assert 1 in results
    assert 3 in results
    # Error item not in results (logged instead)

def test_progress_callbacks():
    """Test that progress callbacks are invoked"""
    # Arrange
    mock_progress = Mock()
    executor = ParallelExecutor(max_workers=2)
    items = [1, 2, 3]

    # Act
    executor.execute_parallel(items, lambda x: x, progress=mock_progress)

    # Assert
    mock_progress.started.assert_called_once_with(3)
    assert mock_progress.completed.call_count == 3
    mock_progress.finish.assert_called_once()
```

**Integration Tests (reduced):**

```python
# tests/test_scanner_integration.py

"""Integration tests - verify components work together"""

def test_full_scan_workflow_with_real_components():
    """Integration test with real cache + API + threading"""
    # This test uses REAL components (not mocks)
    # Validates that ScanOrchestrator + ParallelExecutor compose correctly

    # Setup
    cache_manager = CacheManager(cache_dir=tmp_path)

    # ... test implementation ...

    # Fewer of these needed now that we have unit tests
```

#### Migration Plan

1. **Create new modules** (protocols, scan_orchestrator, parallel_executor)
2. **Write unit tests** for new modules (achieve 95%+ coverage)
3. **Update scanner.py** to use new composition
4. **Run full test suite** - verify no regressions
5. **Remove complex integration tests** that are now covered by unit tests
6. **Update documentation** - architecture diagrams

#### Expected Impact

**Coverage:**
- scanner.py: 71.03% → 85%+ (+14%)
- New modules: 95%+ coverage

**Test Changes:**
- **Add:** ~20 unit tests for ScanOrchestrator
- **Add:** ~15 unit tests for ParallelExecutor
- **Remove:** ~40 complex integration tests
- **Net:** -5 tests with better coverage

**Code Quality:**
- ✅ Separated concerns (SRP)
- ✅ Testable business logic (no side effects)
- ✅ Reusable threading pattern
- ✅ Clear dependency injection

**Maintainability:**
- Unit tests run in <1s (vs integration tests 10s+)
- Easy to add new functionality
- Clear architecture boundaries

---

### 2.2 Property-Based Retry Testing

**Priority:** HIGH
**Effort:** 6-8 hours
**Risk:** Low (test-only changes)

#### Current Problem

**48 retry tests** across 4 files testing similar scenarios:

| File | Tests | Focus |
|------|-------|-------|
| test_retry.py | 18 | Basic retry mechanism |
| test_retry_deterministic.py | 10 | Deterministic behavior |
| test_retry_analysis.py | 8 | Retry analysis |
| test_workflow_retry.py | 12 | Workflow-level retries |
| **Total** | **48** | |

**Example of Duplication:**

```python
# test_retry.py
def test_retry_on_500_error():
    # Test retry with status 500
    ...

def test_retry_on_502_error():
    # Test retry with status 502
    ...

def test_retry_on_503_error():
    # Test retry with status 503
    ...

# 15 more similar tests for different status codes
```

**Problems:**
- Repetitive test cases
- Manual enumeration of edge cases
- High maintenance burden
- Missing edge cases

#### Proposed Solution: Hypothesis Property Tests

Replace 30-40 example tests with 10-15 property tests using **Hypothesis**:

```python
# tests/test_retry_properties.py

"""Property-based tests for retry mechanism using Hypothesis"""

import pytest
from hypothesis import given, strategies as st, settings
from hypothesis.stateful import RuleBasedStateMachine, rule, invariant
from unittest.mock import Mock, patch
from vscode_scanner.vscan_api import VscanAPIClient

# ============================================================================
# Property 1: Retry Count Never Exceeds Max Retries
# ============================================================================

@given(
    status_codes=st.lists(
        st.integers(min_value=500, max_value=599),
        min_size=1,
        max_size=10
    ),
    max_retries=st.integers(min_value=1, max_value=5)
)
@settings(max_examples=1000)  # Hypothesis generates 1000 test scenarios
def test_retry_respects_max_retries(status_codes, max_retries):
    """
    PROPERTY: Number of retry attempts never exceeds max_retries

    Hypothesis will generate:
    - Different failure sequences
    - Different max_retry values
    - Edge cases we didn't think of
    """
    # Arrange
    mock_session = Mock()
    mock_response = Mock()
    mock_response.status_code = status_codes[0]
    mock_response.raise_for_status.side_effect = Exception("Server Error")
    mock_session.get.return_value = mock_response

    client = VscanAPIClient(max_retries=max_retries)

    # Act
    with pytest.raises(Exception):
        with patch.object(client, '_session', mock_session):
            client.scan_extension("test", "extension")

    # Assert - PROPERTY HOLDS
    actual_attempts = mock_session.get.call_count
    assert actual_attempts <= max_retries + 1  # Initial + retries

# ============================================================================
# Property 2: Exponential Backoff Grows Correctly
# ============================================================================

@given(
    retry_count=st.integers(min_value=0, max_value=5),
    base_delay=st.floats(min_value=0.1, max_value=2.0),
    jitter_seed=st.integers(min_value=0, max_value=1000)
)
def test_backoff_grows_exponentially(retry_count, base_delay, jitter_seed):
    """
    PROPERTY: Backoff delay grows exponentially with jitter

    delay = base_delay * (2 ^ retry_count) + jitter
    """
    from vscode_scanner.vscan_api import _calculate_backoff

    # Act
    delay = _calculate_backoff(retry_count, base_delay, jitter_seed)

    # Assert - PROPERTY HOLDS
    min_expected = base_delay * (2 ** retry_count)
    max_expected = min_expected * 1.5  # Max jitter factor

    assert min_expected <= delay <= max_expected

# ============================================================================
# Property 3: Successful Response Stops Retries
# ============================================================================

@given(
    success_after=st.integers(min_value=0, max_value=4),
    max_retries=st.integers(min_value=1, max_value=5)
)
def test_success_stops_retries(success_after, max_retries):
    """
    PROPERTY: If request succeeds after N retries, no more retries occur
    """
    # Arrange
    call_count = 0

    def mock_request(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        if call_count <= success_after:
            raise Exception("Temporary error")
        return {"scan_status": "success"}

    client = VscanAPIClient(max_retries=max_retries)

    # Act
    if success_after <= max_retries:
        # Should succeed
        result = client.scan_extension("test", "extension")
        assert result["scan_status"] == "success"
        assert call_count == success_after + 1
    else:
        # Should fail (not enough retries)
        with pytest.raises(Exception):
            client.scan_extension("test", "extension")

# ============================================================================
# Property 4: Retryable vs Non-Retryable Errors
# ============================================================================

@given(
    status_code=st.integers(min_value=400, max_value=599),
)
def test_retry_decision_is_consistent(status_code):
    """
    PROPERTY: Error classification is consistent

    - 5xx errors: always retryable
    - 429 (rate limit): retryable
    - 4xx (except 429): not retryable
    """
    from vscode_scanner.vscan_api import _is_retryable_error

    # Act
    is_retryable = _is_retryable_error(status_code)

    # Assert - PROPERTY HOLDS
    if 500 <= status_code <= 599:
        assert is_retryable is True
    elif status_code == 429:
        assert is_retryable is True
    elif 400 <= status_code <= 499:
        assert is_retryable is False

# ============================================================================
# Property 5: Stateful Testing - Retry State Machine
# ============================================================================

class RetryStateMachine(RuleBasedStateMachine):
    """
    Stateful property testing - models retry state transitions

    Hypothesis will explore ALL possible state paths:
    - Initial → Retry → Success
    - Initial → Retry → Retry → Failure
    - etc.
    """

    def __init__(self):
        super().__init__()
        self.attempt_count = 0
        self.max_retries = 3
        self.succeeded = False
        self.failed = False

    @rule()
    def make_request(self):
        """Simulate a request attempt"""
        if self.succeeded or self.failed:
            return  # Terminal state

        self.attempt_count += 1

        if self.attempt_count > self.max_retries:
            self.failed = True

    @rule()
    def succeed(self):
        """Simulate successful response"""
        if not self.failed:
            self.succeeded = True

    @invariant()
    def check_invariants(self):
        """Properties that ALWAYS hold"""
        # Invariant 1: Can't succeed and fail
        assert not (self.succeeded and self.failed)

        # Invariant 2: Attempt count never exceeds max+1
        assert self.attempt_count <= self.max_retries + 1

        # Invariant 3: If succeeded, attempts <= max_retries
        if self.succeeded:
            assert self.attempt_count <= self.max_retries + 1

# Run stateful test
TestRetryStateMachine = RetryStateMachine.TestCase
```

#### Implementation Plan

1. **Install Hypothesis** (already in dependencies)
2. **Create property tests** for retry mechanism
3. **Run property tests** - Hypothesis generates 1000+ scenarios each
4. **Identify redundant example tests**
5. **Remove redundant tests** (~30 tests)
6. **Keep essential example tests** (~18 tests for documentation)
7. **Update test documentation** - explain property-based approach

#### Expected Impact

**Test Changes:**
- **Add:** ~10 property tests
- **Remove:** ~30 example tests
- **Net:** -20 tests

**Coverage:**
- **Same or better** - properties cover edge cases we missed
- Hypothesis generates 1000+ scenarios per property
- Example: `test_retry_respects_max_retries` generates 1000 different:
  - Failure sequences (500, 502, 503, 504, etc.)
  - Max retry values (1, 2, 3, 4, 5)
  - Edge cases (empty lists, max retries = 1, etc.)

**Maintainability:**
- **Fewer tests to maintain**
- **Better coverage** of edge cases
- **Self-documenting** - properties describe behavior
- **Finds bugs** - Hypothesis found edge cases in many projects

---

### 2.3 Extract CLI Validation Logic

**Priority:** MEDIUM
**Effort:** 4-6 hours (REDUCED - Phase 0 simplified output mode validation)
**Risk:** Medium (API changes)

#### Benefits from Phase 0

**Phase 0 Simplifications:**
- Removed `plain` parameter validation (one fewer flag to test)
- Simpler output mode logic (only `quiet` flag remains)
- Fewer parameter combinations to validate
- Reduced from 25 to 24 parameters

#### Current Problem

[cli.py](../../vscode_scanner/cli.py) has **67.55% coverage** with **~540 statements** (after Phase 0):

```python
@app.command()
def scan(
    # ... 24 parameters (plain flag removed) ...
    verified_only: bool = typer.Option(False, ...),
    unverified_only: bool = typer.Option(False, ...),
    with_vulnerabilities: bool = typer.Option(False, ...),
    without_vulnerabilities: bool = typer.Option(False, ...),
    quiet: bool = typer.Option(False, "--quiet", "-q"),  # Only output flag now
):
    """Scan installed VS Code extensions"""

    # VALIDATION LOGIC MIXED WITH FRAMEWORK
    if verified_only and unverified_only:
        console.print(
            "[red]Error: Cannot use both --verified-only and "
            "--unverified-only[/red]"
        )
        raise typer.Exit(2)

    if with_vulnerabilities and without_vulnerabilities:
        console.print(
            "[red]Error: Cannot use both --with-vulnerabilities and "
            "--without-vulnerabilities[/red]"
        )
        raise typer.Exit(2)

    # No output mode validation needed (plain flag removed)

    # BUSINESS LOGIC MIXED WITH FRAMEWORK
    from .config_manager import ConfigManager
    config = ConfigManager()

    workers = workers or config.get('scan.workers', 3)
    delay = delay or config.get('scan.delay', 1.5)
    # ... 20 more lines of config merging ...

    # DYNAMIC IMPORTS INSIDE FUNCTION
    from .scanner import run_scan

    # Execute scan (now passing only `quiet` for output)
    exit_code = run_scan(..., quiet=quiet)  # Simpler!
    raise typer.Exit(exit_code)
```

**Problems:**

1. **Cannot unit test validation** - requires full Typer framework
2. **Cannot unit test config merging** - mixed with CLI layer
3. **Dynamic imports** prevent normal testing
4. **~55 integration tests** required to cover all paths (down from 62)
5. **High complexity** - 24 parameters, ~210 branches (reduced)

#### Proposed Refactoring

**Strategy:** Extract pure functions, thin CLI wrapper

**Step 1: Create validators.py**

```python
# vscode_scanner/validators.py

"""CLI validation logic - pure functions for easy testing"""

from typing import Optional, Dict, Any
from dataclasses import dataclass

class ValidationError(ValueError):
    """Raised when CLI arguments fail validation"""
    pass

@dataclass
class ScanFilters:
    """Validated scan filter configuration"""
    publisher: Optional[str]
    min_risk_level: Optional[str]
    verified_only: bool
    unverified_only: bool
    with_vulnerabilities: bool
    without_vulnerabilities: bool

    def __post_init__(self):
        """Validate filter combinations"""
        errors = []

        # Check mutual exclusivity
        if self.verified_only and self.unverified_only:
            errors.append(
                "Cannot use both --verified-only and --unverified-only"
            )

        if self.with_vulnerabilities and self.without_vulnerabilities:
            errors.append(
                "Cannot use both --with-vulnerabilities and "
                "--without-vulnerabilities"
            )

        # Validate risk level
        if self.min_risk_level:
            valid_levels = ['low', 'medium', 'high', 'critical']
            if self.min_risk_level not in valid_levels:
                errors.append(
                    f"Invalid risk level: {self.min_risk_level}. "
                    f"Must be one of: {', '.join(valid_levels)}"
                )

        if errors:
            raise ValidationError("\n".join(errors))

@dataclass
class ScanConfig:
    """Validated scan configuration"""
    workers: int
    delay: float
    timeout: int
    max_retries: int
    cache_enabled: bool
    extensions_dir: Optional[str]
    output_file: Optional[str]
    filters: ScanFilters

    def __post_init__(self):
        """Validate configuration values"""
        errors = []

        # Validate workers
        if not 1 <= self.workers <= 5:
            errors.append(
                f"Workers must be between 1 and 5, got {self.workers}"
            )

        # Validate delay
        if not 0.5 <= self.delay <= 5.0:
            errors.append(
                f"Delay must be between 0.5 and 5.0 seconds, got {self.delay}"
            )

        # Validate timeout
        if not 10 <= self.timeout <= 300:
            errors.append(
                f"Timeout must be between 10 and 300 seconds, got {self.timeout}"
            )

        if errors:
            raise ValidationError("\n".join(errors))

def merge_config(
    cli_args: Dict[str, Any],
    config_file: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Merge CLI arguments with config file values.

    Priority: CLI args > config file > defaults

    Args:
        cli_args: Arguments from CLI (may contain None values)
        config_file: Configuration from file

    Returns:
        Merged configuration with all values resolved
    """
    defaults = {
        'workers': 3,
        'delay': 1.5,
        'timeout': 30,
        'max_retries': 3,
        'cache_enabled': True,
    }

    merged = {}
    for key, default_value in defaults.items():
        # Priority: CLI > config file > default
        cli_value = cli_args.get(key)
        config_value = config_file.get(f'scan.{key}')

        merged[key] = cli_value if cli_value is not None else (
            config_value if config_value is not None else default_value
        )

    return merged
```

**Step 2: Simplify cli.py**

```python
# vscode_scanner/cli.py

from .validators import ScanFilters, ScanConfig, ValidationError, merge_config
from .config_manager import ConfigManager
from .scanner import run_scan

@app.command()
def scan(
    # ... 25 parameters ...
):
    """Scan installed VS Code extensions"""

    try:
        # Step 1: Validate filters (PURE FUNCTION)
        filters = ScanFilters(
            publisher=publisher,
            min_risk_level=min_risk_level,
            verified_only=verified_only,
            unverified_only=unverified_only,
            with_vulnerabilities=with_vulnerabilities,
            without_vulnerabilities=without_vulnerabilities,
        )

        # Step 2: Load config file
        config_file = ConfigManager()

        # Step 3: Merge configuration (PURE FUNCTION)
        cli_args = {
            'workers': workers,
            'delay': delay,
            'timeout': timeout,
            'max_retries': max_retries,
            'cache_enabled': not no_cache,
        }
        merged = merge_config(cli_args, config_file.get_all())

        # Step 4: Create validated config (PURE FUNCTION)
        config = ScanConfig(
            filters=filters,
            extensions_dir=extensions_dir,
            output_file=output,
            **merged
        )

        # Step 5: Execute scan (BUSINESS LOGIC)
        exit_code = run_scan(config)
        raise typer.Exit(exit_code)

    except ValidationError as e:
        # Handle validation errors
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(2)
```

#### Testing Strategy

**New Unit Tests (validators):**

```python
# tests/test_validators.py

"""Unit tests for CLI validators - pure functions"""

import pytest
from vscode_scanner.validators import (
    ScanFilters,
    ScanConfig,
    ValidationError,
    merge_config
)

# ============================================================================
# ScanFilters Tests
# ============================================================================

def test_valid_filters():
    """Test that valid filter combinations are accepted"""
    filters = ScanFilters(
        publisher="microsoft",
        min_risk_level="high",
        verified_only=True,
        unverified_only=False,
        with_vulnerabilities=True,
        without_vulnerabilities=False
    )
    # Should not raise

def test_filters_verified_mutual_exclusivity():
    """Test that verified_only and unverified_only are mutually exclusive"""
    with pytest.raises(ValidationError, match="Cannot use both"):
        ScanFilters(
            publisher=None,
            min_risk_level=None,
            verified_only=True,
            unverified_only=True,  # Conflict
            with_vulnerabilities=False,
            without_vulnerabilities=False
        )

def test_filters_vulnerability_mutual_exclusivity():
    """Test that vulnerability filters are mutually exclusive"""
    with pytest.raises(ValidationError, match="Cannot use both"):
        ScanFilters(
            publisher=None,
            min_risk_level=None,
            verified_only=False,
            unverified_only=False,
            with_vulnerabilities=True,
            without_vulnerabilities=True  # Conflict
        )

@pytest.mark.parametrize("invalid_level", ["super-high", "very-low", "unknown"])
def test_filters_invalid_risk_level(invalid_level):
    """Test that invalid risk levels are rejected"""
    with pytest.raises(ValidationError, match="Invalid risk level"):
        ScanFilters(
            publisher=None,
            min_risk_level=invalid_level,
            verified_only=False,
            unverified_only=False,
            with_vulnerabilities=False,
            without_vulnerabilities=False
        )

# ============================================================================
# ScanConfig Tests
# ============================================================================

def test_valid_config():
    """Test that valid configuration is accepted"""
    filters = ScanFilters(
        publisher=None,
        min_risk_level=None,
        verified_only=False,
        unverified_only=False,
        with_vulnerabilities=False,
        without_vulnerabilities=False
    )

    config = ScanConfig(
        workers=3,
        delay=1.5,
        timeout=30,
        max_retries=3,
        cache_enabled=True,
        extensions_dir=None,
        output_file=None,
        filters=filters
    )
    # Should not raise

@pytest.mark.parametrize("invalid_workers", [0, 6, -1, 100])
def test_config_invalid_workers(invalid_workers):
    """Test that invalid worker counts are rejected"""
    filters = ScanFilters(...)  # Valid filters

    with pytest.raises(ValidationError, match="Workers must be between"):
        ScanConfig(
            workers=invalid_workers,
            delay=1.5,
            timeout=30,
            max_retries=3,
            cache_enabled=True,
            extensions_dir=None,
            output_file=None,
            filters=filters
        )

# ============================================================================
# merge_config Tests
# ============================================================================

def test_merge_config_cli_priority():
    """Test that CLI args take priority over config file"""
    cli_args = {'workers': 5, 'delay': None}
    config_file = {'scan.workers': 3, 'scan.delay': 2.0}

    merged = merge_config(cli_args, config_file)

    assert merged['workers'] == 5  # CLI wins
    assert merged['delay'] == 2.0  # Config file used

def test_merge_config_defaults():
    """Test that defaults are used when neither CLI nor config provide values"""
    cli_args = {'workers': None, 'delay': None}
    config_file = {}

    merged = merge_config(cli_args, config_file)

    assert merged['workers'] == 3  # Default
    assert merged['delay'] == 1.5  # Default
```

**Reduced Integration Tests (cli):**

```python
# tests/test_cli.py - REDUCED from 62 to ~30 tests

"""Integration tests for CLI - Typer framework integration"""

def test_cli_validation_error_display():
    """Test that validation errors are displayed correctly"""
    # This test uses CliRunner (integration test)
    # But validates DISPLAY, not logic

    result = runner.invoke(app, [
        "scan",
        "--verified-only",
        "--unverified-only"  # Conflict
    ])

    assert result.exit_code == 2
    assert "Cannot use both" in result.output

# Many validation tests moved to test_validators.py (unit tests)
```

#### Expected Impact

**Coverage:**
- cli.py: 67.55% → 80%+ (+13%)
- validators.py: 95%+ coverage (pure functions)

**Test Changes:**
- **Add:** ~25 unit tests for validators
- **Remove:** ~30 integration tests from cli
- **Net:** -5 tests

**Code Quality:**
- ✅ Separated validation from framework
- ✅ Testable business logic
- ✅ Clear error messages
- ✅ Reusable validation logic

---

### Phase 2 Outcomes

**After Phase 2 Completion:**

✅ **Coverage Improvements:**
- scanner.py: 71.03% → 85%+ (+14%)
- cli.py: 67.55% → 80%+ (+13%)
- vscan_api.py: Better edge case coverage
- Overall: 82%+ → 88-90% (+6-8%)

✅ **Code Reduction:**
- Clearer architecture with separated concerns
- Removed duplicate retry tests (-30 tests)
- Simplified CLI with extracted validation

✅ **Test Efficiency:**
- **Total tests:** 831 → ~770 (-61 tests)
- More unit tests, fewer integration tests
- Property tests cover 1000+ scenarios each

✅ **Maintainability:**
- Business logic separated from framework integration
- Pure functions easy to test and reason about
- Generic ParallelExecutor pattern reusable

**Risk Assessment:** ⚠️ MEDIUM
- Architectural changes require careful testing
- Breaking changes in internal APIs
- Requires comprehensive validation before release

---

## Phase 3: Polish (Optimization & Tooling)

**Timeline:** 1-2 weeks
**Effort:** 12 hours
**Risk:** Low
**Dependencies:** Phase 2 complete

### Objective

Optimize development workflow and test infrastructure:
- Reduce test code duplication
- Improve test execution speed
- Better test organization
- Enhanced developer experience

### 3.1 Parameterize Security Tests

**Priority:** MEDIUM
**Effort:** 4-6 hours
**Risk:** Low (refactoring only)

#### Current Problem

~100 security tests with repetitive patterns:

```python
# test_path_validation.py

def test_path_validation_url_encoding():
    with pytest.raises(ValueError):
        validate_path("%2e%2e%2f")

def test_path_validation_parent_traversal():
    with pytest.raises(ValueError):
        validate_path("../../../etc/passwd")

def test_path_validation_system_dirs():
    with pytest.raises(ValueError):
        validate_path("/etc/passwd")

# 18 more similar tests...
```

#### Proposed Solution

Use `@pytest.mark.parametrize`:

```python
# test_path_validation.py

@pytest.mark.parametrize("malicious_path,attack_type", [
    # URL-encoded traversal
    ("%2e%2e%2f", "url_encoded_traversal"),
    ("%2e%2e%5c", "url_encoded_backslash"),

    # Parent directory traversal
    ("../../../etc/passwd", "parent_traversal_unix"),
    ("..\\..\\..\\windows\\system32", "parent_traversal_windows"),

    # Absolute system paths
    ("/etc/passwd", "unix_system_file"),
    ("/bin/bash", "unix_system_binary"),
    ("C:\\Windows\\System32", "windows_system_dir"),

    # Null bytes
    ("test\x00.txt", "null_byte_injection"),

    # Hidden files
    ("~/.ssh/id_rsa", "ssh_key"),

    # Device files (Unix)
    ("/dev/null", "unix_device"),

    # Windows special paths
    ("CON", "windows_reserved"),
    ("PRN", "windows_reserved"),
    ("AUX", "windows_reserved"),

    # Encoded null bytes
    ("%00", "encoded_null_byte"),

    # Double encoding
    ("%252e%252e%252f", "double_encoded"),

    # Mixed encodings
    ("..%2f..%2fetc%2fpasswd", "mixed_encoding"),
])
def test_path_validation_blocks_attacks(malicious_path, attack_type):
    """
    Test that path validation blocks common attack vectors.

    This single parameterized test replaces 15+ individual tests
    while maintaining the same coverage and clarity.
    """
    with pytest.raises(ValueError, match="Invalid path"):
        validate_path(malicious_path)
```

**Implementation Plan:**

1. **Identify test patterns** across security test files
2. **Create parameterized tests** with descriptive parameter names
3. **Group related test cases** by attack category
4. **Verify coverage** remains the same or better
5. **Remove old tests** after validation

**Expected Impact:**
- **Test LOC:** -50% for security tests
- **Maintainability:** Easier to add new attack vectors
- **Clarity:** Better documentation of attack types

---

### 3.2 Test Categorization with Markers

**Priority:** HIGH (for DX)
**Effort:** 2-3 hours
**Risk:** None

#### Current Problem

- All 831 tests run every time (~40s)
- No distinction between fast/slow tests
- Development cycle is slower than necessary
- CI runs unnecessary tests for doc changes

#### Proposed Solution

Use pytest markers to categorize tests:

```python
# pytest.ini or pyproject.toml

[tool.pytest.ini_options]
markers = [
    "unit: Fast unit tests (<1s total)",
    "integration: Integration tests with I/O (5-10s)",
    "slow: Slow tests requiring real resources (10s+)",
    "security: Security-focused tests",
    "parallel: Tests for parallel execution",
    "cache: Cache manager tests",
    "api: API client tests",
    "cli: CLI integration tests",
]
```

**Apply markers to tests:**

```python
# Unit tests - fast, no I/O
@pytest.mark.unit
@pytest.mark.fast
def test_validate_path_blocks_traversal():
    """Fast unit test - no I/O, pure function"""
    with pytest.raises(ValueError):
        validate_path("../../../etc/passwd")

# Integration tests - moderate speed
@pytest.mark.integration
@pytest.mark.cache
def test_cache_manager_stores_results(temp_cache_dir):
    """Integration test - real SQLite database"""
    cache = CacheManager(cache_dir=temp_cache_dir)
    cache.save("test-ext", "1.0.0", {"status": "success"})
    result = cache.get("test-ext", "1.0.0")
    assert result is not None

# Slow tests - real resources
@pytest.mark.slow
@pytest.mark.parallel
def test_parallel_scanning_real_extensions():
    """Slow integration test - real extensions, real API"""
    # Test with actual installed extensions
    ...
```

**Usage:**

```bash
# Development: Fast feedback loop
pytest -m "unit" -v
# Runs 500+ unit tests in <5s

# Pre-commit: Good coverage
pytest -m "not slow" -v
# Runs 750+ tests in ~20s

# CI: Full suite
pytest -v
# All 831 tests in ~40s

# Specific domain
pytest -m "security" -v
# Only security tests

# Combined markers
pytest -m "unit and security" -v
# Fast security tests only
```

**CI Optimization:**

```yaml
# .github/workflows/test.yml

jobs:
  fast-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Run unit tests
        run: pytest -m "unit" -v

  full-tests:
    runs-on: ubuntu-latest
    needs: fast-tests
    if: github.event_name == 'push' || contains(github.event.pull_request.labels.*.name, 'test-full')
    steps:
      - name: Run full test suite
        run: pytest -v
```

**Expected Impact:**
- **Development speed:** 5s feedback loop vs 40s
- **CI efficiency:** Skip expensive tests for doc PRs
- **Developer experience:** Faster iteration cycles

---

### 3.3 Improve Transaction Testing

**Priority:** LOW
**Effort:** 3-4 hours
**Risk:** Low

#### Current Problem

cache_manager.py edge cases in transaction handling:

```python
def commit_batch(self):
    """Commit pending batch operations"""
    try:
        self._batch_connection.commit()
    except KeyboardInterrupt:
        self._batch_connection.rollback()
        raise
    except Exception as e:
        self._batch_connection.rollback()
        raise
```

**Current tests:** Basic happy path, interrupt simulation
**Gap:** Database corruption scenarios, disk full, permission errors

#### Proposed Solution

Fault injection tests:

```python
# tests/test_cache_fault_injection.py

"""Fault injection tests for cache_manager transaction handling"""

import pytest
import sqlite3
from unittest.mock import patch, Mock
from vscode_scanner.cache_manager import CacheManager

def test_commit_batch_handles_disk_full(tmp_path):
    """Test that disk full during commit is handled gracefully"""
    # Arrange
    cache = CacheManager(cache_dir=tmp_path)
    cache.begin_batch()
    cache.save("ext-1", "1.0.0", {"status": "success"})

    # Simulate disk full error
    original_commit = cache._batch_connection.commit
    def disk_full_commit():
        raise sqlite3.OperationalError("database or disk is full")

    cache._batch_connection.commit = disk_full_commit

    # Act & Assert
    with pytest.raises(sqlite3.OperationalError, match="disk is full"):
        cache.commit_batch()

    # Verify rollback occurred
    # (cache should be empty after rollback)
    result = cache.get("ext-1", "1.0.0")
    assert result is None

def test_commit_batch_handles_corruption(tmp_path):
    """Test that database corruption during commit is detected"""
    # Corrupt database file during commit
    ...

def test_concurrent_writes_prevented(tmp_path):
    """Test that concurrent batch operations are prevented"""
    # Attempt to start two batches
    ...
```

**Expected Impact:**
- cache_manager.py: +2-3% coverage
- Better error handling reliability
- Edge case documentation

---

### Phase 3 Outcomes

**After Phase 3 Completion:**

✅ **Coverage Improvements:**
- cache_manager.py: +2-3% from edge case testing
- Overall: 88-90% target achieved

✅ **Test Efficiency:**
- Parameterized security tests: -50% LOC
- Test categorization: 5s dev loop (vs 40s)

✅ **Developer Experience:**
- Fast feedback loop for development
- Selective test execution
- Clear test organization

**Risk Assessment:** ✅ LOW
- All changes are test/tooling improvements
- No production code changes
- Incremental improvements

---

## Success Metrics & Validation

### Coverage Targets

| Module | Current | Target | Change | Status |
|--------|---------|--------|--------|--------|
| **Overall** | 78.94% | 88-90% | +10-12% | 🎯 Target |
| scanner.py | 71.03% | 85%+ | +14% | 🎯 Target |
| cli.py | 67.55% | 80%+ | +13% | 🎯 Target |
| cache_manager.py | 71.10% | 88%+ | +17% | 🎯 Target |
| display.py | 94.90% | 96%+ | +1% | ⭐ Phase 0 simplification |
| vscan_api.py | 74.79% | 78%+ | +3% | 🎯 Target |
| constants.py | 100% | 100% | 0% | ✅ Already Perfect |

### Test Suite Metrics

| Metric | Current | Target | Change | Impact |
|--------|---------|--------|--------|--------|
| **Total Tests** | 831 | ~730 | -101 (-12%) | ⭐ Fewer, better tests (improved from -81) |
| **Test LOC** | 23,288 | ~16,500 | -6,788 (-29%) | ⭐ Less maintenance (improved from -27%) |
| **Test Files** | 54 | ~49 | -5 (-9%) | ✅ Better organization |
| **Unit Tests** | ~400 | ~500 | +100 (+25%) | ✅ More unit tests |
| **Integration Tests** | ~400 | ~230 | -170 (-42%) | ⭐ Fewer integration (improved from -37%) |
| **Property Tests** | 20 | 30 | +10 (+50%) | ✅ Better coverage |

### Code Quality Metrics

| Metric | Current | Target | Change | Impact |
|--------|---------|--------|--------|--------|
| **Production LOC** | ~11,500 | ~11,235 | -265 (-2.3%) | ⭐ Plain mode + migration removed |
| **Cyclomatic Complexity** | High (scanner) | Medium | ↓ 30% | ✅ Simpler code |
| **Code Duplication** | 15% | <5% | ↓ 67% | ✅ DRY principle |
| **Test Duplication** | 20% | <5% | ↓ 75% | ✅ Shared fixtures |
| **Legacy Code (LOC)** | 115 + 150 | 0 | -265 (-100%) | ⭐ Migration + plain mode removed |

### Performance Metrics

| Metric | Current | Target | Change | Impact |
|--------|---------|--------|--------|--------|
| **Full Test Suite** | ~40s | ~35s | -5s (-12%) | ✅ Faster CI |
| **Unit Test Suite** | N/A | ~5s | New | ✅ Fast dev loop |
| **Integration Tests** | ~40s | ~20s | -20s (-50%) | ✅ Fewer tests |
| **Property Tests** | ~5s | ~10s | +5s | ✅ More scenarios |

### Development Velocity

| Metric | Current | Target | Change | Impact |
|--------|---------|--------|--------|--------|
| **Dev Feedback Loop** | 40s | 5s | -35s (-87%) | ✅ 8x faster |
| **CI Build Time** | 3-5min | 2-3min | -1-2min | ✅ Faster PRs |
| **Test Maintenance** | High | Low | ↓ 50% | ✅ Less burden |

---

## Risk Assessment & Mitigation

### High Risk Items

#### 1. Cache Schema Breaking Change (Phase 1)

**Risk:** Users lose cached data
**Severity:** LOW (cache is ephemeral)
**Probability:** HIGH (intentional breaking change)

**Mitigation:**
- ✅ Clear communication in CHANGELOG
- ✅ User-friendly error message explaining regeneration
- ✅ Automatic regeneration (no user action required)
- ✅ Document in README and release notes

**Rollback Strategy:**
- Keep v3.6 branch alive for 1-2 months
- Document downgrade path if needed
- Cache regeneration is automatic on upgrade

---

#### 2. ScanOrchestrator Refactoring (Phase 2)

**Risk:** Regression in scan functionality
**Severity:** HIGH (core feature)
**Probability:** MEDIUM (complex refactoring)

**Mitigation:**
- ✅ Comprehensive unit tests (95%+ coverage) before refactoring
- ✅ Keep existing integration tests during transition
- ✅ Gradual rollout: Feature flag for new code path
- ✅ Parallel testing: Run both old and new paths, compare results
- ✅ Beta testing period before stable release

**Validation Checklist:**
- [ ] All existing tests pass with new code
- [ ] New unit tests achieve 95%+ coverage
- [ ] Integration tests validate composition
- [ ] Manual testing with real extensions
- [ ] Performance benchmarks (no regression)
- [ ] Error handling verified

**Rollback Strategy:**
- Keep old code path behind feature flag
- Document how to disable new code if issues found
- Easy revert commit prepared

---

### Medium Risk Items

#### 3. CLI Validation Extraction (Phase 2)

**Risk:** Changed error messages or behavior
**Severity:** MEDIUM (user-facing)
**Probability:** LOW (well-defined behavior)

**Mitigation:**
- ✅ Test all error messages match existing output
- ✅ Integration tests validate CLI behavior unchanged
- ✅ Document any intentional changes in CHANGELOG
- ✅ Beta release for feedback

---

#### 4. Test Consolidation (Phase 1)

**Risk:** Accidentally remove important test cases
**Severity:** MEDIUM (reduced coverage)
**Probability:** LOW (careful migration)

**Mitigation:**
- ✅ Run coverage before consolidation
- ✅ Run coverage after consolidation
- ✅ Verify identical or better coverage
- ✅ Code review of test migration
- ✅ Git history preserves original tests

---

### Low Risk Items

✅ **Shared Test Fixtures** - Refactoring only, no behavior change
✅ **Property-Based Retry Tests** - Additional tests, not replacements
✅ **Test Categorization** - Tooling improvement, no code changes
✅ **Security Test Parameterization** - Maintain same test cases

---

## Implementation Sequencing

### Recommended Order

```
Phase 0: CLI Simplification ⭐ DO FIRST (Foundation for all other phases)
├─ 0.1 Remove --plain flag
├─ 0.2 Simplify display.py
├─ 0.3 Update tests
├─ 0.4 Update documentation
└─ 0.5 Migration & communication
      ↓
Phase 1: Foundation (Parallel - can be done simultaneously)
├─ 1.1 Cache Schema Simplification (CRITICAL PATH)
├─ 1.2 Consolidate Scanner Tests (EASIER after Phase 0)
└─ 1.3 Shared Test Fixtures
      ↓
Phase 2: Architecture (Sequential - dependencies exist)
├─ 2.1 Extract ScanOrchestrator ⭐ (CRITICAL PATH - do first)
├─ 2.2 Property-Based Retry Testing (depends on 2.1 validation)
└─ 2.3 Extract CLI Validation (SIMPLER after Phase 0)
      ↓
Phase 3: Polish (Parallel - independent improvements)
├─ 3.1 Parameterize Security Tests
├─ 3.2 Test Categorization
└─ 3.3 Transaction Testing
```

### Critical Path

1. **Phase 0** - CLI simplification (foundation for everything)
2. **Phase 1.1** - Cache schema change (blocking for v3.7)
3. **Phase 2.1** - ScanOrchestrator extraction (highest impact)
4. **Validation** - Comprehensive testing before release

**Estimated Timeline:**
- **Phase 0:** 1 week (4-6 hours)
- **Phase 1:** 1-2 weeks (10 hours, down from 12)
- **Phase 2:** 3-4 weeks (22 hours, down from 24)
- **Phase 3:** 2 weeks (12 hours)
- **Total:** 8 weeks (~52 hours)

### Milestone Releases

**v3.7.0-alpha.1** (After Phase 0)
- CLI simplification complete
- --plain flag removed
- Breaking change testing period

**v3.7.0-beta.1** (After Phase 1)
- Cache schema 3.0
- Test consolidation
- Beta testing period

**v3.7.0-rc.1** (After Phase 2.1)
- ScanOrchestrator pattern
- Extended beta testing

**v3.7.0-rc.2** (After Phase 2 complete)
- All Phase 2 refactorings
- Pre-release validation

**v3.7.0** (After Phase 3)
- Full release with all improvements
- Updated documentation
- Migration guide

---

## Dependencies & Prerequisites

### Development Environment

**Required:**
- Python 3.8+
- pytest 7.0+
- hypothesis 6.0+ (property testing)
- pytest-cov (coverage)
- pytest-xdist (parallel testing)

**Recommended:**
- pytest-benchmark (performance testing)
- pytest-timeout (prevent hanging tests)
- pytest-mock (mocking utilities)

### Git Workflow

**Branching Strategy:**
```
main
 ├─ feature/v3.7-cache-schema (Phase 1.1)
 ├─ feature/v3.7-test-consolidation (Phase 1.2-1.3)
 ├─ feature/v3.7-scan-orchestrator (Phase 2.1) ⭐
 ├─ feature/v3.7-property-tests (Phase 2.2)
 ├─ feature/v3.7-cli-validation (Phase 2.3)
 └─ feature/v3.7-test-polish (Phase 3)
```

**Merge Order:**
1. Phase 1 branches → `develop` branch
2. Validate `develop` thoroughly
3. Phase 2 branches → `develop` (sequential)
4. Comprehensive testing
5. Phase 3 branches → `develop`
6. Final validation
7. `develop` → `main` (v3.7.0 release)

### Testing Requirements

**Before Each Merge:**
- [ ] All tests pass (831+ tests)
- [ ] Coverage maintained or improved
- [ ] No architectural violations
- [ ] Security tests pass (0 vulnerabilities)
- [ ] Performance benchmarks (no regression)
- [ ] Documentation updated

**Before Release:**
- [ ] Full regression testing
- [ ] Manual testing with real extensions
- [ ] Performance validation
- [ ] Documentation review
- [ ] CHANGELOG complete
- [ ] Migration guide prepared

---

## Documentation Updates

### Files to Update

**Phase 1:**
- [ ] [CHANGELOG.md](../../CHANGELOG.md) - Breaking changes section
- [ ] [README.md](../../README.md) - Cache behavior explanation
- [ ] [docs/guides/ARCHITECTURE.md](../guides/ARCHITECTURE.md) - Cache design philosophy
- [ ] [docs/project/STATUS.md](STATUS.md) - Update coverage metrics

**Phase 2:**
- [ ] [docs/guides/ARCHITECTURE.md](../guides/ARCHITECTURE.md) - New patterns (ScanOrchestrator, ParallelExecutor)
- [ ] [docs/guides/TESTING.md](../guides/TESTING.md) - Property-based testing guide
- [ ] [docs/guides/TESTING_MOCKING.md](../guides/testing/TESTING_MOCKING.md) - Update canonical mocks
- [ ] [docs/project/STATUS.md](STATUS.md) - Update metrics

**Phase 3:**
- [ ] [docs/guides/TESTING.md](../guides/TESTING.md) - Test categorization guide
- [ ] [docs/contributing/TESTING_CHECKLIST.md](../contributing/TESTING_CHECKLIST.md) - Update checklist
- [ ] [docs/project/STATUS.md](STATUS.md) - Final metrics

**New Documentation:**
- [ ] `docs/guides/MIGRATION_v3.7.md` - Migration guide for v3.7
- [ ] `docs/guides/testing/TESTING_PROPERTY_BASED.md` - Update with new retry properties
- [ ] `docs/archive/summaries/v3.7.0-release-notes.md` - Complete release notes

---

## Success Criteria

### Phase 1 Success

- [ ] cache_manager.py coverage ≥88%
- [ ] Legacy migration code removed (115 lines)
- [ ] Scanner tests consolidated (6 → 3 files)
- [ ] Shared fixtures created and used
- [ ] All tests pass
- [ ] Documentation updated

### Phase 2 Success

- [ ] scanner.py coverage ≥85%
- [ ] cli.py coverage ≥80%
- [ ] ScanOrchestrator pattern implemented and tested (95%+ coverage)
- [ ] Property tests replace 30+ retry tests
- [ ] CLI validation extracted to pure functions
- [ ] Net test reduction: -55 tests
- [ ] All tests pass
- [ ] No performance regression

### Phase 3 Success

- [ ] Overall coverage 88-90%
- [ ] Test suite: ~750 tests (-10%)
- [ ] Test LOC: ~17,000 (-27%)
- [ ] Unit test execution: <5s
- [ ] Security tests parameterized
- [ ] Test categorization implemented
- [ ] Documentation complete

### v3.7.0 Release Success

- [ ] All success criteria met
- [ ] Comprehensive testing complete
- [ ] Beta testing period completed (2+ weeks)
- [ ] No critical bugs reported
- [ ] Performance benchmarks pass
- [ ] Documentation review complete
- [ ] Migration guide published
- [ ] Release notes finalized

---

## Appendix

### A. Key Files Reference

**Production Code:**
- [vscode_scanner/cache_manager.py](../../vscode_scanner/cache_manager.py) - Cache management with HMAC
- [vscode_scanner/scanner.py](../../vscode_scanner/scanner.py) - Core scanning logic
- [vscode_scanner/cli.py](../../vscode_scanner/cli.py) - Typer CLI commands
- [vscode_scanner/vscan_api.py](../../vscode_scanner/vscan_api.py) - API client with retry
- [vscode_scanner/display.py](../../vscode_scanner/display.py) - Rich UI integration

**Test Code:**
- [tests/test_scanner.py](../../tests/test_scanner.py) - Scanner tests (1,042 lines)
- [tests/test_cli.py](../../tests/test_cli.py) - CLI tests (62 tests)
- [tests/test_retry.py](../../tests/test_retry.py) - Retry tests (18 tests)
- [tests/test_cache_manager.py](../../tests/test_cache_manager.py) - Cache tests

**Documentation:**
- [docs/guides/ARCHITECTURE.md](../guides/ARCHITECTURE.md) - 3-layer architecture
- [docs/guides/TESTING.md](../guides/TESTING.md) - Testing strategy
- [docs/project/STATUS.md](STATUS.md) - Current status and metrics

### B. Related Work

**Previous Roadmaps:**
- [v3.6 Testability Refactoring](../archive/plans/v3.6-testability-refactoring-plan.md) - ProgressCallback pattern
- [v3.6 Coverage Improvement](../archive/plans/v3.6-coverage-improvement-roadmap.md) - Property-based testing

**Release Notes:**
- [v3.6.0 Release Notes](../archive/summaries/v3.6.0-release-notes.md) - Previous improvements

### C. References

**External Resources:**
- [Hypothesis Documentation](https://hypothesis.readthedocs.io/) - Property-based testing
- [pytest Markers](https://docs.pytest.org/en/stable/how-to/mark.html) - Test categorization
- [Typer Documentation](https://typer.tiangolo.com/) - CLI framework
- [Architecture Patterns](https://martinfowler.com/architecture/) - Martin Fowler's patterns

### D. Contact & Support

**Questions:**
- Review existing documentation first
- Check GitHub issues for similar questions
- Create new issue with `question` label

**Contributions:**
- Follow [GIT_WORKFLOW.md](../contributing/GIT_WORKFLOW.md)
- Read [CONTRIBUTING.md](../../CONTRIBUTING.md)
- Use appropriate branch naming

---

**Document Status:** ✅ READY FOR IMPLEMENTATION
**Next Steps:** Begin Phase 1.1 (Cache Schema Simplification)
**Estimated Completion:** 8 weeks from start
